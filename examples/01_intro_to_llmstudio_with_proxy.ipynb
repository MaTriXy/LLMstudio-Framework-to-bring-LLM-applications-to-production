{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LLMstudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start proxy server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://0.0.0.0:8001 \n",
      "Running LLMstudio Tracking on http://0.0.0.0:8002 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 322, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: 'async_generator' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 327, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 412, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 315, in app\n",
      "    content = await serialize_response(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 180, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 330, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError(\"'async_generator' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 322, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: 'async_generator' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 327, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 412, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 315, in app\n",
      "    content = await serialize_response(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 180, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 330, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError(\"'async_generator' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 322, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: 'async_generator' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 327, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 412, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 315, in app\n",
      "    content = await serialize_response(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 180, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 330, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError(\"'async_generator' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 322, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: 'async_generator' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 327, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 412, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 315, in app\n",
      "    content = await serialize_response(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 180, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 330, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError(\"'async_generator' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 322, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: 'async_generator' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 327, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 412, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 315, in app\n",
      "    content = await serialize_response(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 180, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 330, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError(\"'async_generator' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 322, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: 'async_generator' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 327, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 412, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 315, in app\n",
      "    content = await serialize_response(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 180, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 330, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError(\"'async_generator' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 322, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: 'async_generator' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 327, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 412, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 315, in app\n",
      "    content = await serialize_response(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 180, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 330, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError(\"'async_generator' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 322, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: 'async_generator' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 327, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "           ^^^^^^^^^\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 412, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 315, in app\n",
      "    content = await serialize_response(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/routing.py\", line 180, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogo/fun/LLMstudio/venv/lib/python3.12/site-packages/fastapi/encoders.py\", line 330, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError(\"'async_generator' object is not iterable\"), TypeError('vars() argument must have __dict__ attribute')]\n"
     ]
    }
   ],
   "source": [
    "from llmstudio.server import start_server\n",
    "start_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to LLMStudio Proxy @ 0.0.0.0:8001\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llmstudio.engine.provider import LLMProxyProvider\n",
    "\n",
    "\n",
    "llm = LLMProxyProvider(provider=\"openai\", host=\"0.0.0.0\", port=\"8001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to LLMStudio Proxy @ 0.0.0.0:8001\n"
     ]
    }
   ],
   "source": [
    "# You can set OPENAI_API_KEY and ANTHROPIC_API_KEY on .env file\n",
    "from llmstudio.llm import ProxyConfig\n",
    "proxy = ProxyConfig(host=\"0.0.0.0\", port=\"8001\")\n",
    "\n",
    "openai = LLM(\"openai\", proxy_config=proxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llmstudio.engine.provider.LLMProxyProvider at 0x10f9c73e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai._provider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (non-stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='bbdd240d-ef4a-4550-b31f-d3758228e9c6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I’m an AI and don’t have a personal name, but you can call me whatever you like! Let me know if there's anything I can help you with.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1729004854, model='gpt-4o', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, chat_input=\"What's your name\", chat_output=\"I’m an AI and don’t have a personal name, but you can call me whatever you like! Let me know if there's anything I can help you with.\", context=[{'role': 'user', 'content': \"What's your name\"}], provider='openai', deployment='gpt-4o-2024-08-06', timestamp=1729004855.494591, parameters={}, metrics={'input_tokens': 4, 'output_tokens': 34, 'total_tokens': 38, 'cost_usd': 0.0005300000000000001, 'latency_s': 0.8200538158416748, 'time_to_first_token_s': 0.4039459228515625, 'inter_token_latency_s': 0.01220102871165556, 'tokens_per_second': 42.68012577208389})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.chat(\"What's your name\", model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='65a23cb5-77fa-44ce-a1c1-2c421aff02c6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm an AI language model created by OpenAI, and I don't have a personal name. You can call me Assistant or any name you prefer! How can I help you today?\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1729004863, model='gpt-4o', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, chat_input=\"What's your name\", chat_output=\"I'm an AI language model created by OpenAI, and I don't have a personal name. You can call me Assistant or any name you prefer! How can I help you today?\", context=[{'role': 'user', 'content': \"What's your name\"}], provider='openai', deployment='gpt-4o-2024-08-06', timestamp=1729004864.533138, parameters={}, metrics={'input_tokens': 4, 'output_tokens': 38, 'total_tokens': 42, 'cost_usd': 0.00059, 'latency_s': 1.1568489074707031, 'time_to_first_token_s': 0.5351388454437256, 'inter_token_latency_s': 0.016760536142297694, 'tokens_per_second': 32.84785053139045})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await openai.achat(\"What's your name\", model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a paragfraph about space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(response):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/fun/LLMstudio/llmstudio/llm/__init__.py:62\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, chat_input, model, is_stream, retries, parameters, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\u001b[38;5;28mself\u001b[39m, chat_input: Any, \n\u001b[1;32m     58\u001b[0m          model: \u001b[38;5;28mstr\u001b[39m, is_stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     59\u001b[0m          retries: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m     60\u001b[0m          parameters: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     61\u001b[0m          \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletionChunk \u001b[38;5;241m|\u001b[39m ChatCompletion:\n\u001b[0;32m---> 62\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_provider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session_local:\n\u001b[1;32m     64\u001b[0m         log \u001b[38;5;241m=\u001b[39m schemas\u001b[38;5;241m.\u001b[39mLogDefaultCreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/fun/LLMstudio/llmstudio/engine/provider.py:52\u001b[0m, in \u001b[0;36mLLMProxyProvider.chat\u001b[0;34m(self, chat_input, model, is_stream, retries, parameters, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m     51\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_data)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "\u001b[0;31mException\u001b[0m: Internal Server Error"
     ]
    }
   ],
   "source": [
    "response = openai.chat(\"Write a paragfraph about space\", model=\"gpt-4o\", is_stream=True)\n",
    "for i, chunk in enumerate(response):\n",
    "    if i%20==0:\n",
    "        print(\"\\n\")\n",
    "    if not chunk.metrics:\n",
    "        print(chunk.chat_output, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(\"\\n\\n## Metrics:\")\n",
    "        pprint(chunk.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "async for chunk in await openai.achat(\"Write a paragfraph about space\", model=\"gpt-4o-mini\", is_stream=True):\n",
    "    if i%20==0:\n",
    "        print(\"\\n\")\n",
    "    if not chunk.metrics:\n",
    "        print(chunk.chat_output, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(\"\\n\\n## Metrics:\")\n",
    "        pprint(chunk.metrics)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = LLM('openai/gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_prompt= [{\"role\": \"system\", \"content\": \"You are a tchill dude from jamaica\"},\n",
    "          {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "          {\"role\": \"assistant\", \"content\": \"Ye man doin fain man!\"},\n",
    "          {\"role\": \"user\", \"content\": \"How is the weather today?\"}]\n",
    "\n",
    "inputs = [\n",
    "    \"What's your name?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Can you sing a song?\",\n",
    "    \"Tell me about yourself.\",\n",
    "    complex_prompt\n",
    "]\n",
    "\n",
    "responses = gpt3.run_batch_chat_coroutine(inputs)\n",
    "responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
