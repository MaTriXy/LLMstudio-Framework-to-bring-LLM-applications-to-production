{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Tracking on http://localhost:50002 Running LLMstudio Engine on http://localhost:50001 \n"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "request.chat_input: Hello\n",
      "request.tools: [{'type': 'function', 'function': {'name': 'get_delivery_date', 'description': \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'\", 'parameters': {'type': 'object', 'properties': {'order_id': {'type': 'string', 'description': \"The customer's order ID.\"}}, 'required': ['order_id'], 'additionalProperties': False}}}]\n",
      "request.functions: None\n",
      "has_tools\n",
      "handle tool response\n",
      "request.chat_input: What is the date of Order number 3?\n",
      "request.tools: [{'type': 'function', 'function': {'name': 'get_delivery_date', 'description': \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'\", 'parameters': {'type': 'object', 'properties': {'order_id': {'type': 'string', 'description': \"The customer's order ID.\"}}, 'required': ['order_id'], 'additionalProperties': False}}}]\n",
      "request.functions: None\n",
      "has_tools\n",
      "handle tool response\n",
      "{'id': 'c6ba1021-fc55-4768-b113-02206aa6ed3c', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': '{\"', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471870, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '9dc54430-4b18-4df7-b20c-b0c85be7ea00', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': 'order', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471870, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '5a5a1377-0d9a-453f-87a2-2ca9044a0550', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': '_id', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471870, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': 'e0fbb33b-de04-468c-a62e-cc7b4c81c0d3', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': '\":', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471870, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': 'ea62ac70-2e25-4328-867d-09d2c260c3c7', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': ' \"', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471870, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '442260b1-e9a4-4294-8a2e-c738f9af60f0', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': '3', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471870, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': 'd12aa9a9-2873-4aac-a224-3fa0590b1a05', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': '\"}', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471870, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': 'bd42ac9a-c8fa-4154-9e92-d067455f7a94', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': 'call_testid1234', 'function': {'arguments': '', 'name': 'get_delivery_date', 'type': 'function'}, 'type': None}]}, 'finish_reason': 'tool_calls', 'index': 0, 'logprobs': None}], 'created': 1723471870, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "Tool Call ID: \n",
      "Tool Call Name: \n",
      "Tool Call Type: function\n",
      "Tool Call Arguments: order_id\": \"3\"}\n",
      "request.chat_input: [{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'Can you buy me a tick to madrid'}]\n",
      "request.tools: [{'type': 'function', 'function': {'name': 'get_departure', 'description': 'Use this to fetch the departure time of a train', 'parameters': {'type': 'object', 'properties': {'ticket_number': {'type': 'string'}}, 'required': ['ticket_number']}}}, {'type': 'function', 'function': {'name': 'cancel_ticket', 'description': 'Use this to cancel a ticket', 'parameters': {'type': 'object', 'properties': {'ticket_number': {'type': 'string'}}, 'required': ['ticket_number']}}}, {'type': 'function', 'function': {'name': 'buy_ticket', 'description': 'Use this to buy a ticket', 'parameters': {'type': 'object', 'properties': {'destination': {'type': 'string'}}, 'required': ['destination']}}}, {'type': 'function', 'function': {'name': 'make_complaint', 'description': 'Use this to forward a complaint to the complaint department', 'parameters': {'type': 'object', 'properties': {'complaint': {'type': 'string'}}, 'required': ['complaint']}}}]\n",
      "request.functions: None\n",
      "has_tools\n",
      "handle tool response\n",
      "{'id': '09ec227a-fea6-4771-a55b-969430db9e91', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': '{\"', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471901, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '11fb8cde-3ac6-45f5-b042-bccd33f3ebae', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': 'destination', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471901, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': 'e1fbf79d-bafb-4416-a6fc-1ad81ea02302', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': '\":', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471901, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '861c512d-7f84-4de7-b30b-e21200de4d95', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': ' \"', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471901, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': 'eeaaef1e-a6f7-4da8-b03e-92f40c368307', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': 'Mad', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471901, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '926b8eb9-2cde-489f-ae39-cd84a723be74', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': 'rid', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471901, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '6b169478-e647-44be-874b-726ee2eba825', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': '', 'function': {'arguments': '\"}', 'name': '', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723471901, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '61fe451a-cb40-462b-bff3-191098a7d973', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': 'call_testid1234', 'function': {'arguments': '', 'name': 'buy_ticket', 'type': 'function'}, 'type': None}]}, 'finish_reason': 'tool_calls', 'index': 0, 'logprobs': None}], 'created': 1723471901, 'model': 'Meta-Llama-3.1-405B-Instruct', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "Tool Call ID: \n",
      "Tool Call Name: \n",
      "Tool Call Type: function\n",
      "Tool Call Arguments: destination\": \"Madrid\"}\n"
     ]
    }
   ],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"light_switch\",\n",
    "        \"description\": \"Turn the lights on or off\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"state\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The desired state of the light, e.g. 'on' or 'off'\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"state\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"play_music\",\n",
    "        \"description\": \"Play a music given a name\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"song_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the song to play\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"song_name\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_humidity_level\",\n",
    "            \"description\": \"Get the current humidity level in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM('azurellama/Meta-Llama-3.1-405B-Instruct')\n",
    "# llm = LLM('openai/gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Make Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Turn on the lights.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The lights are on.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Turn the lights off\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_delivery_date\",\n",
    "            \"description\": \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"order_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The customer's order ID.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"order_id\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='d46834c9-f116-4d84-8c61-e30d2c10104c', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='', function=Function(arguments='order_id\": \"3\"}', name=''), type='function')]))], created=1723471870, model='Meta-Llama-3.1-405B-Instruct', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='What is the date of Order number 3?', chat_output='order_id\": \"3\"}', context=[{'role': 'user', 'content': 'What is the date of Order number 3?'}], provider='azurellama', deployment='Meta-Llama-3.1-405B-Instruct', timestamp=1723471870.484921, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 10, 'output_tokens': 6, 'total_tokens': 16, 'cost_usd': 0.0, 'latency_s': 1.748094081878662, 'time_to_first_token_s': 1.6096539497375488, 'inter_token_latency_s': 0.00019904545375279019, 'tokens_per_second': 4.5764127245385255})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('What is the date of Order number 3?', tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='e5356139-03ac-4851-8ff0-b2ec7afd8582', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\": \"San Francisco, CA\"}', name='get_current_weather'), tool_calls=None))], created=1723465478, model='Meta-Llama-3.1-405B-Instruct', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='What is the current weather in San francisco?', chat_output='{\"location\": \"San Francisco, CA\"}', context=[{'role': 'user', 'content': 'What is the current weather in San francisco?'}], provider='azurellama', deployment='Meta-Llama-3.1-405B-Instruct', timestamp=1723465478.4344661, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 10, 'output_tokens': 9, 'total_tokens': 19, 'cost_usd': 0.0, 'latency_s': 1.9766919612884521, 'time_to_first_token_s': 1.8236827850341797, 'inter_token_latency_s': 0.00013921260833740234, 'tokens_per_second': 5.564852903448827})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('What is the current weather in San francisco?', functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Streaming Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import AzureOpenAI, OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"LLAMA_BASE_URL\"),\n",
    "    api_key=os.getenv(\"LLAMA_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"light_switch\",\n",
    "        \"description\": \"Turn the lights on or off\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"state\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The desired state of the light, e.g. 'on' or 'off'\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"state\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"play_music\",\n",
    "        \"description\": \"Play a music given a name\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"song_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the song to play\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"song_name\"],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tool Prompt\n",
    "Podes editar aqui o formato que queres que o output do Llama seja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolPrompt = \"You have access to the following functions:\\n\\n\"\n",
    "\n",
    "for function in functions:\n",
    "    toolPrompt += f\"Use the function '{function['name']}' to '{function['description']}':\\n\"\n",
    "    toolPrompt += f\"{json.dumps(function)}\\n\\n\"\n",
    "\n",
    "toolPrompt += \"\"\"\n",
    "If you choose to call a function, ONLY reply in the following format with no prefix or suffix:\n",
    "§example_function_name§{{\\\"example_name\\\": \\\"example_value\\\"}}\n",
    "\n",
    "Reminder:\n",
    "- Function calls MUST follow the specified format.\n",
    "- Only call one function at a time\n",
    "- Required parameters MUST be specified.\n",
    "- Put the entire function call reply on one line.\n",
    "- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": 'You are a helpful assistant.'\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": toolPrompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Ok.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you turn on the lights and, Please play the music 'Sail'.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"Llama-31-405B-MDclone-tests\",\n",
    "    messages=messages,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    "    functions=functions\n",
    ")\n",
    "\n",
    "# print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "buffer = \"\"\n",
    "saving = False\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].finish_reason\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:53617 Running LLMstudio Tracking on http://localhost:53618 \n",
      "\n",
      "request.chat_input: [{'role': 'system', 'content': 'You are a helpful AI assistant.'}, {'role': 'user', 'content': 'Can you buy me a ticket for madrid, and tell me when the train depparts?'}, {'role': 'assistant', 'content': None, 'function_call': {'arguments': '{\"destination\":\"madrid\"}', 'name': 'buy_ticket'}}, {'role': 'tool', 'content': 'Bought ticket number 123456', 'name': 'buy_ticket'}]\n",
      "request.tools: None\n",
      "request.chat_input: Get the weather in San Francisco\n",
      "request.tools: [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}}, 'required': ['location']}}}, {'type': 'function', 'function': {'name': 'get_humidity_level', 'description': 'Get the current humidity level in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}}, 'required': ['location']}}}]\n"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM('azurellama/Meta-Llama-3.1-405B-Instruct')\n",
    "# llm = LLM('azure/gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='9e85ee79-0e68-4be4-815f-6be385990554', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm happy to help, but I'm a large language model, I don't have the capability to directly purchase tickets or access real-time information. However, I can guide you through the process.\\n\\nTo buy a ticket to Madrid, I'll need more information from you. Could you please provide me with the following details:\\n\\n1. Your departure city\\n2. Your preferred travel dates\\n3. Your preferred mode of transportation (train, flight, or bus)\\n\\nOnce I have this information, I can provide you with general information on how to book a ticket and offer suggestions on where to find the best deals.\\n\\nRegarding train schedules, I can suggest some websites that provide real-time information on train departures and arrivals. For example, you can check the Renfe website (Spain's national railway operator) or a ticketing website like Trainline. Please let me know if you'd like more information on how to use these resources.\\n\\nPlease provide me with the necessary details, and I'll do my best to assist you!\", role='assistant', function_call=None, tool_calls=None))], created=1723208878, model='Meta-Llama-3-405B-Instruct', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Bought ticket number 123456', chat_output=\"I'm happy to help, but I'm a large language model, I don't have the capability to directly purchase tickets or access real-time information. However, I can guide you through the process.\\n\\nTo buy a ticket to Madrid, I'll need more information from you. Could you please provide me with the following details:\\n\\n1. Your departure city\\n2. Your preferred travel dates\\n3. Your preferred mode of transportation (train, flight, or bus)\\n\\nOnce I have this information, I can provide you with general information on how to book a ticket and offer suggestions on where to find the best deals.\\n\\nRegarding train schedules, I can suggest some websites that provide real-time information on train departures and arrivals. For example, you can check the Renfe website (Spain's national railway operator) or a ticketing website like Trainline. Please let me know if you'd like more information on how to use these resources.\\n\\nPlease provide me with the necessary details, and I'll do my best to assist you!\", context=[{'role': 'system', 'content': 'You are a helpful AI assistant.'}, {'role': 'user', 'content': 'Can you buy me a ticket for madrid, and tell me when the train depparts?'}, {'role': 'assistant', 'content': None, 'function_call': {'arguments': '{\"destination\":\"madrid\"}', 'name': 'buy_ticket'}}, {'role': 'tool', 'content': 'Bought ticket number 123456', 'name': 'buy_ticket'}], provider='azurellama', deployment='Meta-Llama-3.1-405B-Instruct', timestamp=1723208891.170505, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 32, 'output_tokens': 205, 'total_tokens': 237, 'cost_usd': 0.0, 'latency_s': 13.961283206939697, 'time_to_first_token_s': 2.8536429405212402, 'inter_token_latency_s': 0.05317858700613374, 'tokens_per_second': 14.826717353395358})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat([{'role': 'system', 'content': 'You are a helpful AI assistant.'}, \n",
    "        {'role': 'user', 'content': 'Can you buy me a ticket for madrid, and tell me when the train depparts?'}, \n",
    "        {'role': 'assistant', 'content': None, 'function_call': {'arguments': '{\"destination\":\"madrid\"}', 'name': 'buy_ticket'}}, \n",
    "        {'role': 'tool', 'content': 'Bought ticket number 123456', 'name': 'buy_ticket'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import AzureOpenAI, OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_humidity_level\",\n",
    "            \"description\": \"Get the current humidity level in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_API_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the weather level in San Francisco\",\n",
    "        }\n",
    "    ],\n",
    "    tools = tools,\n",
    "    model=\"gpt-4o\",\n",
    "    stream = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9uKMhqGtABFFymWULk21oDcfmpVbC', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_w4VzPgWdKKPJgqAtrkmxesDd', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_current_weather'), type='function')]), content_filter_results={})], created=1723211679, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_36b0c83da2', usage=CompletionUsage(completion_tokens=18, prompt_tokens=109, total_tokens=127), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='', choices=[], created=0, model='', object='', system_fingerprint=None, usage=None, prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_9LyZslj0n6i46ULG4qkNuyIV', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='San', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' Francisco', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=',', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' CA', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='tool_calls', index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'role': 'system', 'content': 'You are a helpful AI assistant.'}, \n",
    "{'role': 'user', 'content': 'Can you buy me a ticket for madrid, and tell me when the train depparts?'}, \n",
    "{'role': 'assistant', 'content': None, 'function_call': {'arguments': '{\"destination\":\"madrid\"}', 'name': 'buy_ticket'}}, \n",
    "{'role': 'tool', 'content': 'Bought ticket number 123456', 'name': 'buy_ticket'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatCompletionChunk(id='chatcmpl-9uI2E7YDcVbm4i6Dya69MRRREI1xc', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_79Aw3EzWahn3pmMjv0OdZxK4', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723202722, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_abc28019ad', usage=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have access to the following tools:\n",
      "\n",
      "Use the function 'get_current_weather' to 'Get the current weather in a given location':\n",
      "Parameters format:\n",
      "{\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "        \"location\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
      "        }\n",
      "    },\n",
      "    \"required\": [\n",
      "        \"location\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "If you choose to call a function, ONLY reply in the following format with no prefix or suffix:\n",
      "§function_name§{{\"param_name\": \"param_value\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def create_conversation_with_tool_prompt(self, message, tools):\n",
    "    # Constructing the tool prompt\n",
    "    tool_prompt = \"You have access to the following tools:\\n\\n\"\n",
    "\n",
    "    for tool in tools:\n",
    "        if tool['type'] == 'function':\n",
    "            func = tool['function']\n",
    "            tool_prompt += f\"Use the function '{func['name']}' to '{func['description']}':\\n\"\n",
    "            # Formatting the parameters to show the user how to use them\n",
    "            params_info = json.dumps(func['parameters'], indent=4)\n",
    "            tool_prompt += f\"Parameters format:\\n{params_info}\\n\\n\"\n",
    "\n",
    "    tool_prompt += \"\"\"\n",
    "If you choose to call a function, ONLY reply in the following format with no prefix or suffix:\n",
    "§function_name§{{\"param_name\": \"param_value\"}}\n",
    "\"\"\"\n",
    "\n",
    "    return tool_prompt\n",
    "\n",
    "# Example usage:\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Assuming this method is part of a class, you'd call it with a message and the tools list.\n",
    "# For standalone testing, you might need to remove 'self' and call it directly.\n",
    "message = \"Hello, let's get started by choosing a function to use.\"\n",
    "prompt_text = create_conversation_with_tool_prompt(None, message, tools)\n",
    "print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('79Aw3EzWahn3pmMjv0OdZxK4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('726546ca-be48-43f0-ad3d-d53276ddfa52')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatCompletionChunk(id='chatcmpl-9uJdkpHzMwQhfZWQTY2Le2iMMmzJL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None, content_filter_results={})], created=1723208892, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_36b0c83da2', usage=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_delivery_date\",\n",
    "            \"description\": \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"order_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The customer's order ID.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"order_id\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='sk-proj-A839WupZs91FZOKSltqXT3BlbkFJo14x1dajcONyA9WGLiVY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "                messages=(\n",
    "                    [{\"role\": \"user\", \"content\": 'What is the date of Order number 3?'}]\n",
    "                    ),\n",
    "                tools = tools,\n",
    "                stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl-9vQ5QFBANu9ndnqGqNuM1K6Rp7zGg', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_CJLNEY36qhmNNH0KOK3R93t5', function=ChoiceDeltaToolCallFunction(arguments='', name='get_delivery_date'), type='function')], refusal=None), finish_reason=None, index=0, logprobs=None)], created=1723472000, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9vQ5QFBANu9ndnqGqNuM1K6Rp7zGg', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723472000, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9vQ5QFBANu9ndnqGqNuM1K6Rp7zGg', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='order', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723472000, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9vQ5QFBANu9ndnqGqNuM1K6Rp7zGg', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='_id', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723472000, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9vQ5QFBANu9ndnqGqNuM1K6Rp7zGg', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723472000, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9vQ5QFBANu9ndnqGqNuM1K6Rp7zGg', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='3', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723472000, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9vQ5QFBANu9ndnqGqNuM1K6Rp7zGg', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723472000, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9vQ5QFBANu9ndnqGqNuM1K6Rp7zGg', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='tool_calls', index=0, logprobs=None)], created=1723472000, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_departure(ticket_number: str):\n",
    "    \"\"\"Use this to fetch the departure time of a train\"\"\"\n",
    "    return \"12:00 AM\"\n",
    "\n",
    "@tool\n",
    "def cancel_ticket(ticket_number: str):\n",
    "    \"\"\"Use this to cancel a ticket\"\"\"\n",
    "    return \"Ticket cancelled\"\n",
    "\n",
    "@tool\n",
    "def buy_ticket(destination: str):\n",
    "    \"\"\"Use this to buy a ticket\"\"\"\n",
    "    return \"Bought ticket number 123456\"\n",
    "\n",
    "@tool\n",
    "def make_complaint(complaint: str):\n",
    "    \"\"\"Use this to forward a complaint to the complaint department\"\"\"\n",
    "    return \"Complaint forwarded. We appreciate your feedback.\"\n",
    "\n",
    "tools = [get_departure, cancel_ticket, buy_ticket, make_complaint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:50001 \n",
      "Running LLMstudio Tracking on http://localhost:50002 \n",
      "input_variables=['agent_scratchpad', 'input'] optional_variables=['chat_history'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "request.tools[{'type': 'function', 'function': {'name': 'get_departure', 'description': 'Use this to fetch the departure time of a train', 'parameters': {'type': 'object', 'properties': {'ticket_number': {'type': 'string'}}, 'required': ['ticket_number']}}}, {'type': 'function', 'function': {'name': 'cancel_ticket', 'description': 'Use this to cancel a ticket', 'parameters': {'type': 'object', 'properties': {'ticket_number': {'type': 'string'}}, 'required': ['ticket_number']}}}, {'type': 'function', 'function': {'name': 'buy_ticket', 'description': 'Use this to buy a ticket', 'parameters': {'type': 'object', 'properties': {'destination': {'type': 'string'}}, 'required': ['destination']}}}, {'type': 'function', 'function': {'name': 'make_complaint', 'description': 'Use this to forward a complaint to the complaint department', 'parameters': {'type': 'object', 'properties': {'complaint': {'type': 'string'}}, 'required': ['complaint']}}}]\n",
      "request.functionsNone\n",
      "Tool Call ID: call_e5Lo8xBXSL4L7oO8WuCeFCPM\n",
      "Tool Call Name: buy_ticket\n",
      "Tool Call Type: None\n",
      "Tool Call Arguments: {\n",
      "  \"destination\": \"madrid\"\n",
      "}\n",
      "Error: 1 validation error for ChatCompletionMessageToolCall\n",
      "type\n",
      "  Input should be 'function' [type=literal_error, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/literal_error\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_openai_tools_agent(llm\u001b[38;5;241m=\u001b[39mmodel, prompt\u001b[38;5;241m=\u001b[39mprompt, tools\u001b[38;5;241m=\u001b[39mtools)\n\u001b[1;32m     14\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan you buy me a tick to madrid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Using with chat history\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# from langchain_core.messages import AIMessage, HumanMessage\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# agent_executor.invoke(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:1432\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1432\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1441\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1442\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:1138\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1138\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1148\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:1138\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1138\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1148\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:1166\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:514\u001b[0m, in \u001b[0;36mRunnableMultiActionAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:3262\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   3257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3258\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   3259\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 3262\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:3249\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   3244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3245\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   3246\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3247\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 3249\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   3250\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m   3252\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   3253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3254\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:2054\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 2054\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   2056\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:3211\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3209\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[0;32m-> 3211\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_pipeline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:1272\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m final: Input\n\u001b[1;32m   1270\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The default implementation of transform is to buffer input and\u001b[39;49;00m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# then call stream.\u001b[39;49;00m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# It'll attempt to gather all input into a single chunk using\u001b[39;49;00m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the `+` operator.\u001b[39;49;00m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If the input is not addable, then we'll assume that we can\u001b[39;49;00m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# only operate on the last chunk,\u001b[39;49;00m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# and we'll iterate until we get to the last chunk.\u001b[39;49;00m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgot_first_val\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:5301\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   5296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5297\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   5298\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5299\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   5300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 5301\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   5302\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5303\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5304\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5305\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:1290\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1290\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:363\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[BaseMessageChunk]:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream(async_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}}):\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;66;03m# model doesn't implement streaming, so use default implementation\u001b[39;00m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cast(\n\u001b[0;32m--> 363\u001b[0m             BaseMessageChunk, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m         config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:756\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    750\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    754\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    755\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:613\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    612\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    614\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    615\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    617\u001b[0m ]\n\u001b[1;32m    618\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:603\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 603\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m         )\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:829\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    826\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m         )\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# Add response metadata to each generation\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result\u001b[38;5;241m.\u001b[39mgenerations):\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/langchain.py:56\u001b[0m, in \u001b[0;36mChatLLMstudio._generate\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[BaseMessage], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m     55\u001b[0m     messages_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, [])\n\u001b[0;32m---> 56\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/__init__.py:72\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, input, is_stream, retries, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         error_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_data)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "\u001b[0;31mException\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "from llmstudio.llm.langchain import ChatLLMstudio\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "model = ChatLLMstudio(model_id='azurellama/Meta-Llama-3.1-405B-Instruct', temperature=0)\n",
    "# model = ChatLLMstudio(model_id='openai/gpt-4', temperature=0)\n",
    "# tools = [get_departure, cancel_ticket, buy_ticket, make_complaint]\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "agent = create_openai_tools_agent(llm=model, prompt=prompt, tools=tools)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"Can you buy me a tick to madrid\"})\n",
    "\n",
    "# Using with chat history\n",
    "# from langchain_core.messages import AIMessage, HumanMessage\n",
    "# agent_executor.invoke(\n",
    "#     {\n",
    "#         \"input\": \"Can you tell me the temperature in San Francisco?\",\n",
    "#         \"chat_history\": [\n",
    "#             HumanMessage(content=\"hi! my name is bob\"),\n",
    "#             AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "#         ],\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
