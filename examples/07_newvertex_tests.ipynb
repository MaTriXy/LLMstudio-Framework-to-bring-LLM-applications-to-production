{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:54809 Running LLMstudio Tracking on http://localhost:54810 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM\n",
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "import requests\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw Vertex Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemini-1.5-flash'\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent?alt=sse\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-goog-api-key\": api_key,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} }]\n",
    "\n",
    "calculator_2 = \"\"\"function_declarations {\n",
    "  name: \"multiply\"\n",
    "  description: \"Returns the product of two numbers.\"\n",
    "  parameters {\n",
    "    type_: OBJECT\n",
    "    properties {\n",
    "      key: \"b\"\n",
    "      value {\n",
    "        type_: NUMBER\n",
    "      }\n",
    "    }\n",
    "    properties {\n",
    "      key: \"a\"\n",
    "      value {\n",
    "        type_: NUMBER\n",
    "      }\n",
    "    }\n",
    "    required: \"a\"\n",
    "    required: \"b\"\n",
    "  }\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'What is 9 times 10?'\n",
    "data = {\n",
    "  \"system_instruction\": {\n",
    "    \"parts\": {\n",
    "      \"text\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"contents\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"parts\": [\n",
    "        {\n",
    "          \"text\": \"How many legs does a spider have?\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, headers=headers, json=data, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data: {\"candidates\": [{\"content\": {\"parts\": [{\"text\": \"A\"}],\"role\": \"model\"},\"finishReason\": \"STOP\",\"index\": 0}],\"usageMetadata\": {\"promptTokenCount\": 9,\"candidatesTokenCount\": 1,\"totalTokenCount\": 10}}\\r\\n\\r\\ndata: {\"candidates\": [{\"content\": {\"parts\": [{\"text\": \" spider has **eight** legs. \\\\n\"}],\"role\": \"model\"},\"finishReason\": \"STOP\",\"index\": 0,\"safetyRatings\": [{\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_HATE_SPEECH\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_HARASSMENT\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\"probability\": \"NEGLIGIBLE\"}]}],\"usageMetadata\": {\"promptTokenCount\": 9,\"candidatesTokenCount\": 8,\"totalTokenCount\": 17}}\\r\\n\\r\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vertex Functions Tests - Str to OAI to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"system_instruction\": {\n",
      "    \"parts\": {\n",
      "      \"text\": \"\"\n",
      "    }\n",
      "  },\n",
      "  \"contents\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"parts\": [\n",
      "        {\n",
      "          \"text\": \"How many legs does a spider have?\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"system_instruction\": {\n",
      "    \"parts\": {\n",
      "      \"text\": \"You are a helpful assistant.\"\n",
      "    }\n",
      "  },\n",
      "  \"contents\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"parts\": [\n",
      "        {\n",
      "          \"text\": \"Hello\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"model\",\n",
      "      \"parts\": [\n",
      "        {\n",
      "          \"text\": \"Hi there!\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Error: Input must be a list of dictionaries, each containing 'role' and 'content' keys.\n"
     ]
    }
   ],
   "source": [
    "def convert_openai_to_vertexai(input_data):\n",
    "    # Check if the input is a simple string\n",
    "    if isinstance(input_data, str):\n",
    "        # Return a Vertex AI formatted message with a user message\n",
    "        return {\n",
    "            \"system_instruction\": {\n",
    "                \"parts\": {\n",
    "                    \"text\": \"\"  # Empty system instruction\n",
    "                }\n",
    "            },\n",
    "            \"contents\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": [{\"text\": input_data}]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # Validate if input_data is a list and each element is a dictionary with the correct structure\n",
    "    if not isinstance(input_data, list) or not all(isinstance(msg, dict) and 'role' in msg and 'content' in msg for msg in input_data):\n",
    "        raise ValueError(\"Input must be a list of dictionaries, each containing 'role' and 'content' keys.\")\n",
    "\n",
    "    # Initialize the Vertex AI format if the input is not a simple string\n",
    "    vertexai_format = {\n",
    "        \"system_instruction\": {\n",
    "            \"parts\": {\n",
    "                \"text\": \"\"\n",
    "            }\n",
    "        },\n",
    "        \"contents\": []\n",
    "    }\n",
    "    \n",
    "    # Loop through the OpenAI formatted messages\n",
    "    for message in input_data:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            # Set the system instruction\n",
    "            vertexai_format[\"system_instruction\"][\"parts\"][\"text\"] = message[\"content\"]\n",
    "        elif message[\"role\"] in [\"user\", \"assistant\"]:\n",
    "            # Convert roles: 'assistant' -> 'model'\n",
    "            role = \"model\" if message[\"role\"] == \"assistant\" else \"user\"\n",
    "            # Append the message to the contents list in Vertex AI format\n",
    "            vertexai_format[\"contents\"].append({\n",
    "                \"role\": role,\n",
    "                \"parts\": [{\"text\": message[\"content\"]}]\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {message['role']}. Expected 'system', 'user', or 'assistant'.\")\n",
    "    \n",
    "    return vertexai_format\n",
    "\n",
    "# Example usage with a string input\n",
    "simple_input = \"How many legs does a spider have?\"\n",
    "vertexai_format = convert_openai_to_vertexai(simple_input)\n",
    "print(json.dumps(vertexai_format, indent=2))\n",
    "\n",
    "# Example usage with a valid list of OpenAI messages\n",
    "openai_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n",
    "]\n",
    "vertexai_format = convert_openai_to_vertexai(openai_messages)\n",
    "print(json.dumps(vertexai_format, indent=2))\n",
    "\n",
    "# Example usage with an invalid input to trigger an error\n",
    "try:\n",
    "    invalid_input = [\n",
    "        {\"role\": \"system\", \"text\": \"You are a helpful assistant.\"}  # 'text' should be 'content'\n",
    "    ]\n",
    "    vertexai_format = convert_openai_to_vertexai(invalid_input)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vertex Chat test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Tracking on http://localhost:54833 Running LLMstudio Engine on http://localhost:54832 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM('newvertex/gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 String format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='8f6573a0-b358-41f3-9c1e-3d4273e42d9d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! 👋  What can I do for you today? 😊 \\n', role='assistant', function_call=None, tool_calls=None))], created=1723653916, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Hello', chat_output='Hello! 👋  What can I do for you today? 😊 \\n', context=[{'role': 'user', 'content': 'Hello'}], provider='newvertex', deployment='gemini-1.5-flash', timestamp=1723653916.200755, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 1, 'output_tokens': 16, 'total_tokens': 17, 'cost_usd': 1.7149999999999997e-05, 'latency_s': 1.1608567237854004, 'time_to_first_token_s': 1.0242607593536377, 'inter_token_latency_s': 0.03346975644429525, 'tokens_per_second': 3.4457310002534407})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 OpenAI format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='af9e0a75-270b-4126-bece-c0dff06cf373', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Wah gwaan, mon? Mi good, thanks fi asking.  How yuhself? 😊  \\n', role='assistant', function_call=None, tool_calls=None))], created=1723654634, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Hello, how are you?', chat_output='Wah gwaan, mon? Mi good, thanks fi asking.  How yuhself? 😊  \\n', context=[{'role': 'system', 'content': 'You are Jamaican.'}, {'role': 'assistant', 'content': 'How can i help you?'}, {'role': 'user', 'content': 'Hello, how are you?'}], provider='newvertex', deployment='gemini-1.5-flash', timestamp=1723654634.440474, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 16, 'output_tokens': 24, 'total_tokens': 40, 'cost_usd': 3.0799999999999996e-05, 'latency_s': 1.1928188800811768, 'time_to_first_token_s': 1.0959320068359375, 'inter_token_latency_s': 0.013440990447998047, 'tokens_per_second': 5.030101468205863})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You are Jamaican.\"},\n",
    "           {\"role\": \"assistant\", \"content\": \"How can i help you?\"},\n",
    "           {\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "\n",
    "llm.chat(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vertex function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = {\n",
    "  \"function_declarations\": [\n",
    "    {\n",
    "      \"name\": \"get_weather\",\n",
    "      \"description\": \"Gets the weather for a certain location.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location we are getting the weather for. For example: San Francisco\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"location\"\n",
    "        ]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"get_time\",\n",
    "      \"description\": \"Gets the current time for a certain location.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location we are getting the current time for. For example: New York\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"location\"  \n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:55418 Running LLMstudio Tracking on http://localhost:55419 \n",
      "\n",
      "{'function_declarations': [{'name': 'get_weather', 'description': 'Gets the weather for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the weather for. For example: San Francisco'}}, 'required': ['location']}}, {'name': 'get_time', 'description': 'Gets the current time for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the current time for. For example: New York'}}, 'required': ['location']}}]}\n",
      "{'system_instruction': {'parts': {'text': 'You are a helpfull assistant'}}, 'contents': [{'role': 'model', 'parts': [{'text': 'How can i help you?'}]}, {'role': 'user', 'parts': [{'text': 'What is the weather in San Francisco?'}]}], 'tools': {'function_declarations': [{'name': 'get_weather', 'description': 'Gets the weather for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the weather for. For example: San Francisco'}}, 'required': ['location']}}, {'name': 'get_time', 'description': 'Gets the current time for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the current time for. For example: New York'}}, 'required': ['location']}}]}, 'tool_config': {'function_calling_config': {'mode': 'ANY'}}}\n",
      "{'system_instruction': {'parts': {'text': 'You are a helpfull assistant'}}, 'contents': [{'role': 'model', 'parts': [{'text': 'How can i help you?'}]}, {'role': 'user', 'parts': [{'text': 'What is the weather in San Francisco?'}]}], 'tools': {'function_declarations': [{'name': 'get_weather', 'description': 'Gets the weather for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the weather for. For example: San Francisco'}}, 'required': ['location']}}, {'name': 'get_time', 'description': 'Gets the current time for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the current time for. For example: New York'}}, 'required': ['location']}}]}, 'tool_config': {'function_calling_config': {'mode': 'ANY'}}}\n",
      "chunk: b'data: {\"candidates\": [{\"content\": {\"parts\": [{\"functionCall\": {\"name\": \"get_weather\",\"args\": {\"location\": \"San Francisco\"}}}],\"role\": \"model\"},\"finishReason\": \"STOP\",\"index\": 0,\"safetyRatings\": [{\"category\": \"HARM_CATEGORY_HARASSMENT\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_HATE_SPEECH\",\"probability\": \"NEGLIGIBLE\"}]}],\"usageMetadata\": {\"promptTokenCount\": 130,\"candidatesTokenCount\": 16,\"totalTokenCount\": 146}}\\r\\n\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('newvertex/gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "async generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m message \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpfull assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      2\u001b[0m            {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can i help you?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      3\u001b[0m            {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather in San Francisco?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/__init__.py:70\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, input, is_stream, retries, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m         error_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_data)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "\u001b[0;31mException\u001b[0m: async generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You are a helpfull assistant\"},\n",
    "           {\"role\": \"assistant\", \"content\": \"How can i help you?\"},\n",
    "           {\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"}]\n",
    "\n",
    "\n",
    "llm.chat(message, tools = functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Raw Vertex function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = {\n",
    "  \"function_declarations\": [\n",
    "    {\n",
    "      \"name\": \"get_weather\",\n",
    "      \"description\": \"Gets the weather for a certain location.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location we are getting the weather for. For example: San Francisco\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"location\"\n",
    "        ]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"get_time\",\n",
    "      \"description\": \"Gets the current time for a certain location.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location we are getting the current time for. For example: New York\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"location\"  \n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_json = json.dumps(functions)\n",
    "functions_obj = json.loads(functions_json)\n",
    "type(functions_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "# Prepare the data for the API call\n",
    "api_key = os.getenv('GOOGLE_API_KEY')  # Replace this with your actual Google API key\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\n",
    "    \"system_instruction\": {\n",
    "        \"parts\": {\n",
    "            \"text\": \"You are a helpful assistant.\"\n",
    "        }\n",
    "    },\n",
    "    \"contents\": {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": {\n",
    "            \"text\": \"Get the weather in San Francisco\"\n",
    "        }\n",
    "    },\n",
    "    \"tools\": functions_obj,  # Use the parsed object instead of the JSON string\n",
    "    \"tool_config\": {\n",
    "        \"function_calling_config\": {\"mode\": \"ANY\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "# Make the API call\n",
    "response = requests.post(f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key={api_key}',\n",
    "                         headers=headers, json=data, stream=True)\n",
    "\n",
    "# Print the response content\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tools_payload(openai_tools):\n",
    "    # Check if the tools list is not None and not empty\n",
    "    if not openai_tools:\n",
    "        return None\n",
    "    \n",
    "    # Initialize the VertexAI format dictionary\n",
    "    vertexai_format = {\"function_declarations\": []}\n",
    "\n",
    "    # Loop through each tool in the OpenAI tools list\n",
    "    for tool in openai_tools:\n",
    "        if tool['type'] == 'function':  # Ensure we are converting function types\n",
    "            function = tool['function']\n",
    "            # Create a dictionary for the function declaration\n",
    "            vertexai_function = {\n",
    "                \"name\": function['name'],\n",
    "                \"description\": function['description'],\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": function['parameters'].get('required', [])\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Handle each parameter and its properties\n",
    "            for param_name, param_details in function['parameters']['properties'].items():\n",
    "                vertexai_function[\"parameters\"][\"properties\"][param_name] = {\n",
    "                    \"type\": param_details['type'],\n",
    "                    \"description\": param_details.get('description', '')  # Optional description\n",
    "                }\n",
    "\n",
    "            # Append the function declaration to the VertexAI declarations list\n",
    "            vertexai_format['function_declarations'].append(vertexai_function)\n",
    "\n",
    "    return vertexai_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'function_declarations': [{'name': 'get_departure', 'description': 'Use this to fetch the departure time of a train', 'parameters': {'type': 'object', 'properties': {'ticket_number': {'type': 'string', 'description': ''}}, 'required': ['ticket_number']}}, {'name': 'get_weather', 'description': 'Fetches weather information for a specified location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': ''}}, 'required': ['location']}}, {'name': 'calculate_tax', 'description': 'Calculates tax based on income and state', 'parameters': {'type': 'object', 'properties': {'income': {'type': 'number', 'description': ''}, 'state': {'type': 'string', 'description': ''}}, 'required': ['income', 'state']}}]}\n"
     ]
    }
   ],
   "source": [
    "# Example tools in OpenAI format\n",
    "openai_tools = [\n",
    "    {\n",
    "        'type': 'function', \n",
    "        'function': {\n",
    "            'name': 'get_departure', \n",
    "            'description': 'Use this to fetch the departure time of a train', \n",
    "            'parameters': {\n",
    "                'type': 'object', \n",
    "                'properties': {\n",
    "                    'ticket_number': {\n",
    "                        'type': 'string'\n",
    "                    }\n",
    "                }, \n",
    "                'required': ['ticket_number']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'type': 'function', \n",
    "        'function': {\n",
    "            'name': 'get_weather', \n",
    "            'description': 'Fetches weather information for a specified location', \n",
    "            'parameters': {\n",
    "                'type': 'object', \n",
    "                'properties': {\n",
    "                    'location': {\n",
    "                        'type': 'string'\n",
    "                    }\n",
    "                }, \n",
    "                'required': ['location']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'type': 'function', \n",
    "        'function': {\n",
    "            'name': 'calculate_tax', \n",
    "            'description': 'Calculates tax based on income and state', \n",
    "            'parameters': {\n",
    "                'type': 'object', \n",
    "                'properties': {\n",
    "                    'income': {\n",
    "                        'type': 'number'\n",
    "                    },\n",
    "                    'state': {\n",
    "                        'type': 'string'\n",
    "                    }\n",
    "                }, \n",
    "                'required': ['income', 'state']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function and print the result\n",
    "converted_tools_payload = create_tools_payload(openai_tools)\n",
    "print(converted_tools_payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Dict, Optional, Union\n",
    "\n",
    "# Define Pydantic model for OpenAI tool parameter\n",
    "class OpenAIParameter(BaseModel):\n",
    "    type: str\n",
    "    description: Optional[str] = None\n",
    "\n",
    "# Define Pydantic model for OpenAI tool parameters container\n",
    "class OpenAIParameters(BaseModel):\n",
    "    type: str\n",
    "    properties: Dict[str, OpenAIParameter]\n",
    "    required: List[str]\n",
    "\n",
    "# Define Pydantic model for OpenAI tool function\n",
    "class OpenAIToolFunction(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: OpenAIParameters\n",
    "\n",
    "# Define Pydantic model for OpenAI tool\n",
    "class OpenAITool(BaseModel):\n",
    "    type: str\n",
    "    function: OpenAIToolFunction\n",
    "\n",
    "# Define Pydantic model for VertexAI tool parameter\n",
    "class VertexAIParameter(BaseModel):\n",
    "    type: str\n",
    "    description: str\n",
    "\n",
    "# Define Pydantic model for VertexAI tool parameters container\n",
    "class VertexAIParameters(BaseModel):\n",
    "    type: str\n",
    "    properties: Dict[str, VertexAIParameter]\n",
    "    required: List[str]\n",
    "\n",
    "# Define Pydantic model for VertexAI function declaration\n",
    "class VertexAIFunctionDeclaration(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: VertexAIParameters\n",
    "\n",
    "# Define Pydantic model for VertexAI functions container\n",
    "class VertexAI(BaseModel):\n",
    "    function_declarations: List[VertexAIFunctionDeclaration]\n",
    "\n",
    "def process_tools(tools: Optional[Union[List[Dict], Dict]]) -> Optional[VertexAI]:\n",
    "    if tools is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Try to parse as OpenAI format\n",
    "        parsed_tools = [OpenAITool(**tool) for tool in tools] if isinstance(tools, list) else [OpenAITool(**tools)]\n",
    "        # Convert to VertexAI format\n",
    "        function_declarations = []\n",
    "        for tool in parsed_tools:\n",
    "            function = tool.function\n",
    "            properties = {\n",
    "                name: VertexAIParameter(type=param.type, description=param.description or \"\")\n",
    "                for name, param in function.parameters.properties.items()\n",
    "            }\n",
    "            function_decl = VertexAIFunctionDeclaration(\n",
    "                name=function.name,\n",
    "                description=function.description,\n",
    "                parameters=VertexAIParameters(\n",
    "                    type=function.parameters.type,\n",
    "                    properties=properties,\n",
    "                    required=function.parameters.required\n",
    "                )\n",
    "            )\n",
    "            function_declarations.append(function_decl)\n",
    "        return VertexAI(function_declarations=function_declarations).model_dump()\n",
    "    except ValidationError:\n",
    "        # If the format is not OpenAI, attempt to validate as VertexAI format\n",
    "        try:\n",
    "            return VertexAI(**tools).model_dump()\n",
    "        except ValidationError:\n",
    "            # If it fails to validate as VertexAI, throw an error\n",
    "            raise ValueError(\"Invalid tool format. Tool data must be in OpenAI or VertexAI format.\")\n",
    "\n",
    "# Example usage with your preferred format data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_declarations': [{'name': 'get_weather',\n",
       "   'description': 'Gets the weather for a certain location.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'location': {'type': 'string',\n",
       "      'description': 'The location we are getting the weather for. For example: San Francisco'}},\n",
       "    'required': ['location']}},\n",
       "  {'name': 'get_time',\n",
       "   'description': 'Gets the current time for a certain location.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'location': {'type': 'string',\n",
       "      'description': 'The location we are getting the current time for. For example: New York'}},\n",
       "    'required': ['location']}}]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tools(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
