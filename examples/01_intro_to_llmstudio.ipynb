{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LLMstudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set OPENAI_API_KEY and ANTHROPIC_API_KEY on .env file\n",
    "\n",
    "claude2 = LLM(\"anthropic/claude-2.1\") # or you can pass api_key as argument here\n",
    "gpt4 = LLM(\"openai/gpt-4-1106-preview\")\n",
    "gpt3 = LLM(\"openai/gpt-3.5-turbo\")\n",
    "flash = LLM(\"vertexai/gemni-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (non-stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: http://localhost:50001/api/engine/chat/openai",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgpt4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms your name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/__init__.py:50\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, input, is_stream, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m, is_stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_HOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_PORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/engine/chat/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m         json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     48\u001b[0m     )\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http://localhost:50001/api/engine/chat/openai"
     ]
    }
   ],
   "source": [
    "gpt4.chat(\"What's your name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a1c2004a-fb10-4847-86fe-ca18ff3e57e6',\n",
       " 'chat_input': \"What's your name\",\n",
       " 'chat_output': \"I'm an AI language model created by OpenAI, known as ChatGPT. I don't have a personal name since I'm not a human, but you can refer to me based on my model name if you like! How can I assist you today?\",\n",
       " 'timestamp': 1705588697.5492659,\n",
       " 'provider': 'openai',\n",
       " 'model': 'gpt-4-1106-preview',\n",
       " 'metrics': {'input_tokens': 4,\n",
       "  'output_tokens': 53,\n",
       "  'total_tokens': 57,\n",
       "  'cost': 0.0016300000000000002,\n",
       "  'latency': 4.192759990692139,\n",
       "  'time_to_first_token': 0.624748945236206,\n",
       "  'inter_token_latency': 0.06729131824565383,\n",
       "  'tokens_per_second': 12.87934442226103},\n",
       " 'parameters': {'temperature': 1,\n",
       "  'max_tokens': 256,\n",
       "  'top_p': 1,\n",
       "  'frequency_penalty': 0,\n",
       "  'presence_penalty': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await gpt4.async_chat(\"What's your name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space is vast and mysterious, holding endless possibilities and uncharted territories. It is a vast expanse that stretches beyond our wildest imagination and comprehension. With billions of galaxies, each comprised of billions of stars, it is a place where wonders and mysteries reside. From the birth of stars to the formation of black holes, space has been a subject of fascination for scientists, philosophers, and dreamers alike. Moreover, space holds the potential for extraterrestrial life, as our search for habitable planets continues, fueling our curiosity and desire to unveil the secrets of the universe."
     ]
    }
   ],
   "source": [
    "response = gpt3.chat(\"Write a paragfraph about space\", is_stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi back!"
     ]
    }
   ],
   "source": [
    "async for chunk in await claude2.async_chat(\"Say hi back\", is_stream=True):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What's your name?\n",
      "Response: My name is Assistant. How can I help you today?\n",
      "\n",
      "Input: Tell me a joke.\n",
      "Response: Why was the math book sad?\n",
      "\n",
      "Because it had too many problems.\n",
      "\n",
      "Input: What's the weather like?\n",
      "Response: I'm sorry, but I am an AI and do not have the capability to provide real-time weather information. You can check the weather on a weather website or app for the most up-to-date information.\n",
      "\n",
      "Input: Can you sing a song?\n",
      "Response: I'm sorry, I can't sing as I am a text-based AI. But I can provide you with lyrics if you'd like!\n",
      "\n",
      "Input: Tell me about yourself.\n",
      "Response: As an AI assistant, I am here to provide information and assistance on a variety of topics. I am constantly learning and improving to better serve your needs. I am programmed to provide accurate information and help with any questions you may have. Feel free to ask me anything, and I will do my best to assist you.\n",
      "\n",
      "Input: What's the capital of France?\n",
      "Response: Paris\n",
      "\n",
      "Input: How tall is the Eiffel Tower?\n",
      "Response: The Eiffel Tower stands at a height of 1,063 feet (324 meters) including antennas.\n",
      "\n",
      "Input: What's the population of Tokyo?\n",
      "Response: As of 2021, the estimated population of Tokyo is around 14 million people.\n",
      "\n",
      "Input: Who is the president of the United States?\n",
      "Response: As of October 2021, the President of the United States is Joe Biden.\n",
      "\n",
      "Input: What's the largest ocean?\n",
      "Response: The largest ocean in the world is the Pacific Ocean.\n",
      "\n",
      "Input: How many continents are there?\n",
      "Response: There are seven continents: Africa, Antarctica, Asia, Europe, North America, Australia (Oceania), and South America.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the inputs\n",
    "inputs = [\n",
    "    \"What's your name?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Can you sing a song?\",\n",
    "    \"Tell me about yourself.\",\n",
    "    \"What's the capital of France?\",\n",
    "    \"How tall is the Eiffel Tower?\",\n",
    "    \"What's the population of Tokyo?\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"What's the largest ocean?\",\n",
    "    \"How many continents are there?\",\n",
    "]\n",
    "\n",
    "\n",
    "# Call the batch_chat function\n",
    "responses = gpt3.batch_chat(inputs, num_threads=10)\n",
    "\n",
    "# Print the responses\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Input: {inputs[i]}\")\n",
    "    print(f\"Response: {response.chat_output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
