{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LLMstudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio_core import LLM\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set OPENAI_API_KEY and ANTHROPIC_API_KEY on .env file\n",
    "\n",
    "anthropic = LLM(\"anthropic\") # or you can pass api_key as an argument here\n",
    "openai = LLM(\"openai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (non-stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='0388f750-9076-42bd-a963-d4eab24bb1ba', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I’m called ChatGPT. How can I assist you today?', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1728478112, model='gpt-4o', object='chat.completion', service_tier=None, system_fingerprint='fp_2f406b9113', usage=CompletionUsage(completion_tokens=13, prompt_tokens=10, total_tokens=23, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0), prompt_tokens_details={'cached_tokens': 0}), chat_input=\"What's your name\", chat_output='I’m called ChatGPT. How can I assist you today?', context=[{'role': 'user', 'content': \"What's your name\"}], provider='openai', deployment='gpt-4o-2024-08-06', timestamp=1728478112.735258, parameters={}, metrics={'input_tokens': 10, 'output_tokens': 13, 'total_tokens': 23, 'cost_usd': 0.000245, 'latency_s': 1.051265001296997, 'time_to_first_token_s': None, 'inter_token_latency_s': None, 'tokens_per_second': 21.878403610529958})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.chat(\"What's your name\", model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='7f196166-935d-4539-a1a9-ba9ca32ef62a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I’m an AI developed by OpenAI and don’t have a personal name, but you can call me Assistant. How can I help you today?', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1728478113, model='gpt-4o', object='chat.completion', service_tier=None, system_fingerprint='fp_e5e4913e83', usage=CompletionUsage(completion_tokens=30, prompt_tokens=10, total_tokens=40, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0), prompt_tokens_details={'cached_tokens': 0}), chat_input=\"What's your name\", chat_output='I’m an AI developed by OpenAI and don’t have a personal name, but you can call me Assistant. How can I help you today?', context=[{'role': 'user', 'content': \"What's your name\"}], provider='openai', deployment='gpt-4o-2024-08-06', timestamp=1728478113.728288, parameters={}, metrics={'input_tokens': 10, 'output_tokens': 30, 'total_tokens': 40, 'cost_usd': 0.0005, 'latency_s': 0.9794328212738037, 'time_to_first_token_s': None, 'inter_token_latency_s': None, 'tokens_per_second': 40.83996281437445})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await openai.achat(\"What's your name\", model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Space, an awe-inspiring expanse that stretches beyond the grasp of human imagination, is the\n",
      "\n",
      " vast, seemingly infinite void that envelops planets, stars, galaxies, and all celestial bodies. It\n",
      "\n",
      " serves as both the birthplace and the playground of cosmic phenomena, from the swirling, fiery dance of stars\n",
      "\n",
      " being born in nebulae to the mysterious, unseen forces of dark matter and dark energy that constitute\n",
      "\n",
      " the universe's unseen framework. This vast universe is not only a subject of scientific inquiry and discovery but\n",
      "\n",
      " also a source of inspiration and wonder. Human space exploration endeavors reflect our innate curiosity and longing to understand\n",
      "\n",
      " our place in the cosmos, driving the development of advanced technology and international cooperation. Space, with its\n",
      "\n",
      " breathtaking beauty and infinite mysteries, challenges our understanding and continually beckons us to explore further and dream bigger\n",
      "\n",
      ".\n",
      "\n",
      "## Metrics:\n",
      "{'cost_usd': 0.002455,\n",
      " 'input_tokens': 8,\n",
      " 'inter_token_latency_s': 0.010445242342741592,\n",
      " 'latency_s': 2.4184927940368652,\n",
      " 'output_tokens': 161,\n",
      " 'time_to_first_token_s': 0.7361259460449219,\n",
      " 'tokens_per_second': 66.98386714214482,\n",
      " 'total_tokens': 169}\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat(\"Write a paragfraph about space\", model=\"gpt-4o\", is_stream=True)\n",
    "for i, chunk in enumerate(response):\n",
    "    if i%20==0:\n",
    "        print(\"\\n\")\n",
    "    if chunk.chat_output is not None:\n",
    "        print(chunk.chat_output, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(\"\\n\\n## Metrics:\")\n",
    "        pprint(chunk.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi! How can I assist you today?\n",
      "\n",
      "## Metrics:\n",
      "{'cost_usd': 5.85e-06,\n",
      " 'input_tokens': 3,\n",
      " 'inter_token_latency_s': 0.01939098834991455,\n",
      " 'latency_s': 0.8029687404632568,\n",
      " 'output_tokens': 9,\n",
      " 'time_to_first_token_s': 0.6087050437927246,\n",
      " 'tokens_per_second': 13.699163424037863,\n",
      " 'total_tokens': 12}\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "async for chunk in await openai.achat(\"Say hi back\", model=\"gpt-4o-mini\", is_stream=True):\n",
    "    if i%20==0:\n",
    "        print(\"\\n\")\n",
    "    if chunk.chat_output is not None:\n",
    "        print(chunk.chat_output, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(\"\\n\\n## Metrics:\")\n",
    "        pprint(chunk.metrics)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llmstudio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllmstudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llmstudio'"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = LLM('openai/gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletion(id='bce77107-790e-4194-b709-c1f973908e13', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am a virtual assistant and I do not have a personal name. You can simply refer to me as \"assistant\". How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1718357127, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your name?\", chat_output='I am a virtual assistant and I do not have a personal name. You can simply refer to me as \"assistant\". How can I assist you today?', context=[{'role': 'user', 'content': \"What's your name?\"}], provider='openai', timestamp=1718357130.3642838, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 31, 'total_tokens': 36, 'cost_usd': 4.9e-05, 'latency_s': 2.8777408599853516, 'time_to_first_token_s': 2.8716890811920166, 'inter_token_latency_s': 8.686631917953491e-05, 'tokens_per_second': 11.46732857668358}),\n",
       " ChatCompletion(id='f5058611-d4e5-48cc-810f-5cfbf82bd70f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why couldn't the bicycle find its way home? Because it lost its bearings!\", role='assistant', function_call=None, tool_calls=None))], created=1718357129, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me a joke.', chat_output=\"Why couldn't the bicycle find its way home? Because it lost its bearings!\", context=[{'role': 'user', 'content': 'Tell me a joke.'}], provider='openai', timestamp=1718357130.392538, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 16, 'total_tokens': 21, 'cost_usd': 2.65e-05, 'latency_s': 0.9708259105682373, 'time_to_first_token_s': 0.9693810939788818, 'inter_token_latency_s': 7.653236389160156e-05, 'tokens_per_second': 18.540914291692484}),\n",
       " ChatCompletion(id='bb195dce-30b4-4836-a9bc-9d73ea81efcc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, I am an AI and do not have the ability to determine the current weather. You can check the weather in your area by using a weather app on your phone or by visiting a weather website.\", role='assistant', function_call=None, tool_calls=None))], created=1718357129, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's the weather like?\", chat_output=\"I'm sorry, I am an AI and do not have the ability to determine the current weather. You can check the weather in your area by using a weather app on your phone or by visiting a weather website.\", context=[{'role': 'user', 'content': \"What's the weather like?\"}], provider='openai', timestamp=1718357130.386699, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 43, 'total_tokens': 49, 'cost_usd': 6.75e-05, 'latency_s': 1.5981099605560303, 'time_to_first_token_s': 1.594458818435669, 'inter_token_latency_s': 7.893822409889914e-05, 'tokens_per_second': 28.158262641916803}),\n",
       " ChatCompletion(id='2e689f44-7977-4a21-9b45-4ac9fcf17e0e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm an AI assistant, so I don't have a physical voice to sing with. However, I can provide lyrics or information about songs if you'd like! Just let me know how I can assist you.\", role='assistant', function_call=None, tool_calls=None))], created=1718357128, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Can you sing a song?', chat_output=\"I'm an AI assistant, so I don't have a physical voice to sing with. However, I can provide lyrics or information about songs if you'd like! Just let me know how I can assist you.\", context=[{'role': 'user', 'content': 'Can you sing a song?'}], provider='openai', timestamp=1718357130.378707, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 43, 'total_tokens': 49, 'cost_usd': 6.75e-05, 'latency_s': 2.229912042617798, 'time_to_first_token_s': 2.22599196434021, 'inter_token_latency_s': 8.436224677345969e-05, 'tokens_per_second': 20.180168159086847}),\n",
       " ChatCompletion(id='257fb034-90c5-4d98-8ff7-1830ff3a0954', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am an AI assistant designed to provide information and assistance in various topics. I am constantly learning and improving my abilities to help users with their queries and tasks. I am programmed to provide accurate and reliable information based on the data available to me. If you have any specific questions or need assistance, feel free to ask!', role='assistant', function_call=None, tool_calls=None))], created=1718357130, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me about yourself.', chat_output='I am an AI assistant designed to provide information and assistance in various topics. I am constantly learning and improving my abilities to help users with their queries and tasks. I am programmed to provide accurate and reliable information based on the data available to me. If you have any specific questions or need assistance, feel free to ask!', context=[{'role': 'user', 'content': 'Tell me about yourself.'}], provider='openai', timestamp=1718357132.529829, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 64, 'total_tokens': 69, 'cost_usd': 9.850000000000001e-05, 'latency_s': 2.184094190597534, 'time_to_first_token_s': 0.6790041923522949, 'inter_token_latency_s': 0.023148767764751728, 'tokens_per_second': 30.21847697051171}),\n",
       " ChatCompletion(id='813d3a00-2765-479c-b158-bbf916232706', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"De sun be shinin' bright and de breeze be cool, perfect weather for jammin' on de beach! How 'bout you, how's de weather by you?\", role='assistant', function_call=None, tool_calls=None))], created=1718357131, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='How is the weather today?', chat_output=\"De sun be shinin' bright and de breeze be cool, perfect weather for jammin' on de beach! How 'bout you, how's de weather by you?\", context=[{'role': 'system', 'content': 'You are a tchill dude from jamaica'}, {'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': 'Ye man doin fain man!'}, {'role': 'user', 'content': 'How is the weather today?'}], provider='openai', timestamp=1718357132.537878, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 31, 'output_tokens': 35, 'total_tokens': 66, 'cost_usd': 6.8e-05, 'latency_s': 1.5229389667510986, 'time_to_first_token_s': 1.5202741622924805, 'inter_token_latency_s': 6.913476520114475e-05, 'tokens_per_second': 24.29512988227787})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_prompt= [{\"role\": \"system\", \"content\": \"You are a tchill dude from jamaica\"},\n",
    "          {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "          {\"role\": \"assistant\", \"content\": \"Ye man doin fain man!\"},\n",
    "          {\"role\": \"user\", \"content\": \"How is the weather today?\"}]\n",
    "\n",
    "inputs = [\n",
    "    \"What's your name?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Can you sing a song?\",\n",
    "    \"Tell me about yourself.\",\n",
    "    complex_prompt\n",
    "]\n",
    "\n",
    "responses = gpt3.run_batch_chat_coroutine(inputs)\n",
    "responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
