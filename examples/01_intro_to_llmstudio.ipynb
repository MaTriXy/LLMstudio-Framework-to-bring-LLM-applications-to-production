{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LLMstudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set OPENAI_API_KEY and ANTHROPIC_API_KEY on .env file\n",
    "\n",
    "# claude2 = LLM(\"anthropic/claude-2.1\") # or you can pass api_key as argument here\n",
    "# flash = LLM(\"vertexai/gemni-1.5-flash\")\n",
    "# gpt3 = LLM(\"openai/gpt-3.5-turbo\")\n",
    "gpt4 = LLM(\"openai/gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = LLM(\"azure/llama3\", base_url=\"https://Meta-Llama-3-8B-llmstudio-serverless.eastus2.inference.ai.azure.com\", api_key='pfSCzsEG5kiBh9y9QJJacFTeGimml7p8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: http://localhost:50001/api/engine/chat/azure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/__init__.py:53\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, input, is_stream, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m, is_stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_HOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_PORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/engine/chat/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m         json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m         headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     51\u001b[0m     )\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http://localhost:50001/api/engine/chat/azure"
     ]
    }
   ],
   "source": [
    "llama.chat('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (non-stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='40da1768-942f-4433-bbad-c48dace10c85', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm OpenAI's language model called GPT-3. I don't have a personal name because I'm an artificial intelligence.\", role='assistant', function_call=None, tool_calls=None))], created=1718357336, model='gpt-4', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your name\", chat_output=\"I'm OpenAI's language model called GPT-3. I don't have a personal name because I'm an artificial intelligence.\", context=[{'role': 'user', 'content': \"What's your name\"}], provider='openai', timestamp=1718357338.400716, parameters={'temperature': None, 'max_tokens': None, 'top_p': None, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 4, 'output_tokens': 27, 'total_tokens': 31, 'cost_usd': 0.0017400000000000002, 'latency_s': 1.9352331161499023, 'time_to_first_token_s': 0.8234610557556152, 'inter_token_latency_s': 0.03969763857977731, 'tokens_per_second': 14.985274775420736})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4.chat(\"What's your name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a1c2004a-fb10-4847-86fe-ca18ff3e57e6',\n",
       " 'chat_input': \"What's your name\",\n",
       " 'chat_output': \"I'm an AI language model created by OpenAI, known as ChatGPT. I don't have a personal name since I'm not a human, but you can refer to me based on my model name if you like! How can I assist you today?\",\n",
       " 'timestamp': 1705588697.5492659,\n",
       " 'provider': 'openai',\n",
       " 'model': 'gpt-4-1106-preview',\n",
       " 'metrics': {'input_tokens': 4,\n",
       "  'output_tokens': 53,\n",
       "  'total_tokens': 57,\n",
       "  'cost': 0.0016300000000000002,\n",
       "  'latency': 4.192759990692139,\n",
       "  'time_to_first_token': 0.624748945236206,\n",
       "  'inter_token_latency': 0.06729131824565383,\n",
       "  'tokens_per_second': 12.87934442226103},\n",
       " 'parameters': {'temperature': 1,\n",
       "  'max_tokens': 256,\n",
       "  'top_p': 1,\n",
       "  'frequency_penalty': 0,\n",
       "  'presence_penalty': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await gpt4.async_chat(\"What's your name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space is vast and mysterious, holding endless possibilities and uncharted territories. It is a vast expanse that stretches beyond our wildest imagination and comprehension. With billions of galaxies, each comprised of billions of stars, it is a place where wonders and mysteries reside. From the birth of stars to the formation of black holes, space has been a subject of fascination for scientists, philosophers, and dreamers alike. Moreover, space holds the potential for extraterrestrial life, as our search for habitable planets continues, fueling our curiosity and desire to unveil the secrets of the universe."
     ]
    }
   ],
   "source": [
    "response = gpt3.chat(\"Write a paragfraph about space\", is_stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi back!"
     ]
    }
   ],
   "source": [
    "async for chunk in await claude2.async_chat(\"Say hi back\", is_stream=True):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = LLM('openai/gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletion(id='bce77107-790e-4194-b709-c1f973908e13', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am a virtual assistant and I do not have a personal name. You can simply refer to me as \"assistant\". How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1718357127, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your name?\", chat_output='I am a virtual assistant and I do not have a personal name. You can simply refer to me as \"assistant\". How can I assist you today?', context=[{'role': 'user', 'content': \"What's your name?\"}], provider='openai', timestamp=1718357130.3642838, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 31, 'total_tokens': 36, 'cost_usd': 4.9e-05, 'latency_s': 2.8777408599853516, 'time_to_first_token_s': 2.8716890811920166, 'inter_token_latency_s': 8.686631917953491e-05, 'tokens_per_second': 11.46732857668358}),\n",
       " ChatCompletion(id='f5058611-d4e5-48cc-810f-5cfbf82bd70f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why couldn't the bicycle find its way home? Because it lost its bearings!\", role='assistant', function_call=None, tool_calls=None))], created=1718357129, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me a joke.', chat_output=\"Why couldn't the bicycle find its way home? Because it lost its bearings!\", context=[{'role': 'user', 'content': 'Tell me a joke.'}], provider='openai', timestamp=1718357130.392538, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 16, 'total_tokens': 21, 'cost_usd': 2.65e-05, 'latency_s': 0.9708259105682373, 'time_to_first_token_s': 0.9693810939788818, 'inter_token_latency_s': 7.653236389160156e-05, 'tokens_per_second': 18.540914291692484}),\n",
       " ChatCompletion(id='bb195dce-30b4-4836-a9bc-9d73ea81efcc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, I am an AI and do not have the ability to determine the current weather. You can check the weather in your area by using a weather app on your phone or by visiting a weather website.\", role='assistant', function_call=None, tool_calls=None))], created=1718357129, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's the weather like?\", chat_output=\"I'm sorry, I am an AI and do not have the ability to determine the current weather. You can check the weather in your area by using a weather app on your phone or by visiting a weather website.\", context=[{'role': 'user', 'content': \"What's the weather like?\"}], provider='openai', timestamp=1718357130.386699, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 43, 'total_tokens': 49, 'cost_usd': 6.75e-05, 'latency_s': 1.5981099605560303, 'time_to_first_token_s': 1.594458818435669, 'inter_token_latency_s': 7.893822409889914e-05, 'tokens_per_second': 28.158262641916803}),\n",
       " ChatCompletion(id='2e689f44-7977-4a21-9b45-4ac9fcf17e0e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm an AI assistant, so I don't have a physical voice to sing with. However, I can provide lyrics or information about songs if you'd like! Just let me know how I can assist you.\", role='assistant', function_call=None, tool_calls=None))], created=1718357128, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Can you sing a song?', chat_output=\"I'm an AI assistant, so I don't have a physical voice to sing with. However, I can provide lyrics or information about songs if you'd like! Just let me know how I can assist you.\", context=[{'role': 'user', 'content': 'Can you sing a song?'}], provider='openai', timestamp=1718357130.378707, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 43, 'total_tokens': 49, 'cost_usd': 6.75e-05, 'latency_s': 2.229912042617798, 'time_to_first_token_s': 2.22599196434021, 'inter_token_latency_s': 8.436224677345969e-05, 'tokens_per_second': 20.180168159086847}),\n",
       " ChatCompletion(id='257fb034-90c5-4d98-8ff7-1830ff3a0954', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am an AI assistant designed to provide information and assistance in various topics. I am constantly learning and improving my abilities to help users with their queries and tasks. I am programmed to provide accurate and reliable information based on the data available to me. If you have any specific questions or need assistance, feel free to ask!', role='assistant', function_call=None, tool_calls=None))], created=1718357130, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me about yourself.', chat_output='I am an AI assistant designed to provide information and assistance in various topics. I am constantly learning and improving my abilities to help users with their queries and tasks. I am programmed to provide accurate and reliable information based on the data available to me. If you have any specific questions or need assistance, feel free to ask!', context=[{'role': 'user', 'content': 'Tell me about yourself.'}], provider='openai', timestamp=1718357132.529829, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 64, 'total_tokens': 69, 'cost_usd': 9.850000000000001e-05, 'latency_s': 2.184094190597534, 'time_to_first_token_s': 0.6790041923522949, 'inter_token_latency_s': 0.023148767764751728, 'tokens_per_second': 30.21847697051171}),\n",
       " ChatCompletion(id='813d3a00-2765-479c-b158-bbf916232706', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"De sun be shinin' bright and de breeze be cool, perfect weather for jammin' on de beach! How 'bout you, how's de weather by you?\", role='assistant', function_call=None, tool_calls=None))], created=1718357131, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='How is the weather today?', chat_output=\"De sun be shinin' bright and de breeze be cool, perfect weather for jammin' on de beach! How 'bout you, how's de weather by you?\", context=[{'role': 'system', 'content': 'You are a tchill dude from jamaica'}, {'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': 'Ye man doin fain man!'}, {'role': 'user', 'content': 'How is the weather today?'}], provider='openai', timestamp=1718357132.537878, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 31, 'output_tokens': 35, 'total_tokens': 66, 'cost_usd': 6.8e-05, 'latency_s': 1.5229389667510986, 'time_to_first_token_s': 1.5202741622924805, 'inter_token_latency_s': 6.913476520114475e-05, 'tokens_per_second': 24.29512988227787})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_prompt= [{\"role\": \"system\", \"content\": \"You are a tchill dude from jamaica\"},\n",
    "          {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "          {\"role\": \"assistant\", \"content\": \"Ye man doin fain man!\"},\n",
    "          {\"role\": \"user\", \"content\": \"How is the weather today?\"}]\n",
    "\n",
    "inputs = [\n",
    "    \"What's your name?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Can you sing a song?\",\n",
    "    \"Tell me about yourself.\",\n",
    "    complex_prompt\n",
    "]\n",
    "\n",
    "responses = gpt3.run_batch_chat_coroutine(inputs)\n",
    "responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
