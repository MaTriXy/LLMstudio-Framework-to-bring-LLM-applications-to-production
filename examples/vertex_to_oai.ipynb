{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator_gemni = {'function_declarations': [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} },\n",
    "      {'name': 'send_mail',\n",
    "       'description': 'Sends an email to a specified address.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'to': {'type_': 'STRING'},\n",
    "         'subject': {'type_': 'STRING'},\n",
    "         'body': {'type_': 'STRING'} },\n",
    "       'required': ['to', 'subject', 'body']} },\n",
    "      {'name': 'turn_lights',\n",
    "       'description': 'Turns the lights on or off.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'state': {'type_': 'BOOLEAN'} },\n",
    "       'required': ['state']} }\n",
    "    ]\n",
    "}\n",
    "\n",
    "calculator_oai = [\n",
    "    {\n",
    "        \"name\": \"multiply\",\n",
    "        \"description\": \"Returns the product of two numbers.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number.\"\n",
    "                },\n",
    "                \"b\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"turn_lights\",\n",
    "        \"description\": \"Turns the lights on or off.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"state\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"The desired state of the lights.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"state\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_mail\",\n",
    "        \"description\": \"Sends an email to a specified address.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"to\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The recipient's email address.\"\n",
    "                },\n",
    "                \"subject\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The subject of the email.\"\n",
    "                },\n",
    "                \"body\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The body of the email.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"to\", \"subject\", \"body\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time  # for measuring time duration of API calls\n",
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='', name='send_mail'), role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='{\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='to', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='example', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='@gmail', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.com', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\",\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='subject', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='Regarding', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' Important', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' Information', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\",\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='body', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='Hello', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=',\\\\', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\\\\n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='I', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' hope', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' this', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' email', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' finds', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' you', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' well', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' I', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' am', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' writing', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' to', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' share', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' some', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' important', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' information', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' with', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' you', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' Please', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' let', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' me', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' know', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' if', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' you', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' have', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' any', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' questions', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.\\\\', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\\\\n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='Best', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' regards', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=',\\\\', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='[', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='Your', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' Name', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=']', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\"}', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "# a ChatCompletion request\n",
    "openai_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write an email to example@gmail.com. Make up the rest.\"}\n",
    "    ],\n",
    "    functions = calculator_oai,\n",
    "    function_call={\"name\": \"send_mail\"},\n",
    "    temperature=0,\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in openai_response:\n",
    "    print(chunk)\n",
    "    # print(chunk.choices[0].delta.content)\n",
    "    print(\"****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9m0eYkLEDEreH1et3AnYJHnlsitU4', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"state\":true}', name='turn_lights'), tool_calls=None))], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=154, total_tokens=169))\n"
     ]
    }
   ],
   "source": [
    "# a ChatCompletion request\n",
    "openai_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Turn the lights on\"}\n",
    "    ],\n",
    "    functions = calculator_oai,\n",
    "    temperature=0,\n",
    "    stream=False  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatCompletion(id='chatcmpl-9m0eYkLEDEreH1et3AnYJHnlsitU4', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"state\":true}', name='turn_lights'), tool_calls=None))], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=154, total_tokens=169))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "# a ChatCompletion request\n",
    "openai_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in openai_response:\n",
    "    print(chunk)\n",
    "    # print(chunk.choices[0].delta.content)\n",
    "    print(\"****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertexai Function + Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash', tools=[calculator_gemni])\n",
    "response = model.generate_content('whats 9 times 10', stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[function_call {\n",
      "  name: \"multiply\"\n",
      "  args {\n",
      "    fields {\n",
      "      key: \"b\"\n",
      "      value {\n",
      "        number_value: 10\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"a\"\n",
      "      value {\n",
      "        number_value: 9\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "multiply\n",
      "{'a': number_value: 9\n",
      ", 'b': number_value: 10\n",
      "}\n",
      "{\"a\": 9, \"b\": 10}\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from openai.types.chat import ChatCompletionChunk\n",
    "from openai.types.chat.chat_completion_chunk import Choice, ChoiceDelta\n",
    "from openai.types.chat.chat_completion_chunk import ChoiceDeltaFunctionCall\n",
    "# from openai.types.chat import FunctionCall\n",
    "import uuid\n",
    "import re\n",
    "import json\n",
    "\n",
    "def convert_to_json(custom_str):\n",
    "    # Remove whitespace and newlines\n",
    "    custom_str = custom_str.replace('\\n', '').replace(' ', '')\n",
    "    \n",
    "    # Use regex to find key-value pairs\n",
    "    matches = re.findall(r\"'(\\w+)':number_value:(\\d+)\", custom_str)\n",
    "    \n",
    "    # Convert matches to a dictionary\n",
    "    result_dict = {key: int(value) for key, value in matches}\n",
    "    \n",
    "    # Convert dictionary to JSON string\n",
    "    json_str = json.dumps(result_dict)\n",
    "    return json_str\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.parts)\n",
    "    # print(chunk.parts[0].function_call.name)\n",
    "\n",
    "    name = chunk.parts[0].__dict__['_pb'].function_call.name\n",
    "    print(name)\n",
    "    args = chunk.parts[0].__dict__['_pb'].function_call.args.fields\n",
    "    args_str1 = str(chunk.parts[0].__dict__['_pb'].function_call.args.fields)\n",
    "    print(args_str1)\n",
    "    arg_str2 = convert_to_json(args_str1)\n",
    "    print(arg_str2)\n",
    "    # for i in args:\n",
    "    #     print(chunk.parts[0].__dict__['_pb'].function_call.args.fields[i])\n",
    "    # print(args)\n",
    "    \n",
    "\n",
    "\n",
    "    # Construct string\n",
    "    \n",
    "\n",
    "    # ChatCompletionChunk(\n",
    "    #     id=str(uuid.uuid4()),\n",
    "    #     choices=[\n",
    "    #         Choice(\n",
    "    #             delta=ChoiceDelta(function_call=FunctionCall(arguments=arg_str2, name=name)),\n",
    "    #             finish_reason=None,\n",
    "    #             index=0,\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     created=int(time.time()),\n",
    "    #     model='gemini-1.5-flash',\n",
    "    #     object=\"chat.completion.chunk\",\n",
    "    # ).model_dump()\n",
    "    \n",
    "    print(\"_\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def convert_to_json(custom_str):\n",
    "    # Remove whitespace and newlines\n",
    "    custom_str = custom_str.replace('\\n', '').replace(' ', '')\n",
    "    \n",
    "    # Use regex to find key-value pairs\n",
    "    matches = re.findall(r\"'(\\w+)':number_value:(\\d+)\", custom_str)\n",
    "    \n",
    "    # Convert matches to a dictionary\n",
    "    result_dict = {key: int(value) for key, value in matches}\n",
    "    \n",
    "    # Convert dictionary to JSON string\n",
    "    json_str = json.dumps(result_dict)\n",
    "    return json_str\n",
    "\n",
    "# Example usage\n",
    "custom_str = \"{'a': number_value: 9, 'b': number_value: 10}\"\n",
    "json_str = convert_to_json(custom_str)\n",
    "print(json_str)  # Output: {\"a\": 9, \"b\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. OpenAI format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function + Streaming\n",
    "\n",
    "****************\n",
    "**First chunk**\n",
    "\n",
    "ChatCompletionChunk(id='chatcmpl-9ly0H0yO7PorUjPyBKUXW04c6ZpQ7', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='', name='send_mail'), role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721219097, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
    "****************\n",
    "\n",
    "****************\n",
    "**Middle Chunk**\n",
    "\n",
    "ChatCompletionChunk(id='chatcmpl-9ly0H0yO7PorUjPyBKUXW04c6ZpQ7', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='{\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721219097, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
    "****************\n",
    "\n",
    "****************\n",
    "**Final Chunk**\n",
    "\n",
    "ChatCompletionChunk(id='chatcmpl-9ly0H0yO7PorUjPyBKUXW04c6ZpQ7', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1721219097, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
    "****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function\n",
    "\n",
    "****************\n",
    "ChatCompletion(id='chatcmpl-9lvzlofzJbCBtghCboLCxQLeGi86J', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"a\":6,\"b\":7}', name='multiply'), tool_calls=None))], created=1721211377, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=84, total_tokens=93))\n",
    "****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gemini format\n",
    "It's always in the same format. Stream or not stream, it does not make a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function_call {\n",
    "  name: \"multiply\"\n",
    "  args {\n",
    "    fields {\n",
    "      key: \"b\"\n",
    "      value {\n",
    "        number_value: 10\n",
    "      }\n",
    "    }\n",
    "    fields {\n",
    "      key: \"a\"\n",
    "      value {\n",
    "        number_value: 9\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functions\n",
    "1. Multiply a by b\n",
    "2. Send email with to, subject and body\n",
    "3. Turn on and of the light with a boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_functions = {'function_declarations': [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} },\n",
    "      {'name': 'send_mail',\n",
    "       'description': 'Sends an email to a specified address.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'to': {'type_': 'STRING'},\n",
    "         'subject': {'type_': 'STRING'},\n",
    "         'body': {'type_': 'STRING'} },\n",
    "       'required': ['to', 'subject', 'body']} },\n",
    "      {'name': 'turn_lights',\n",
    "       'description': 'Turns the lights on or off.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'state': {'type_': 'BOOLEAN'} },\n",
    "       'required': ['state']} }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_functions = [\n",
    "    {\n",
    "        \"name\": \"multiply\",\n",
    "        \"description\": \"Returns the product of two numbers.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number.\"\n",
    "                },\n",
    "                \"b\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"turn_lights\",\n",
    "        \"description\": \"Turns the lights on or off.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"state\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"The desired state of the lights.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"state\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_mail\",\n",
    "        \"description\": \"Sends an email to a specified address.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"to\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The recipient's email address.\"\n",
    "                },\n",
    "                \"subject\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The subject of the email.\"\n",
    "                },\n",
    "                \"body\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The body of the email.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"to\", \"subject\", \"body\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vertex to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Make Chat call with functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Setup model with tools\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel('gemini-1.5-flash', tools=[oai_functions])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "prompts = ['Whats 0.9 times 10?',\n",
    "           'Send an email to diogoazevedo15@gmail.com. Send hello in the body and make the subject be hello as well.',\n",
    "           'Turn of the light.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get response\n",
    "response = model.generate_content(prompts[1], stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Convert chatcall from VertexAI to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletionChunk, ChatCompletion\n",
    "from openai.types.chat.chat_completion_chunk import Choice, ChoiceDelta\n",
    "from openai.types.chat.chat_completion_chunk import ChoiceDeltaFunctionCall\n",
    "# from openai.types.chat import FunctionCall\n",
    "from openai.types.chat.chat_completion_message import ChatCompletionMessage, FunctionCall\n",
    "import uuid\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def parse_string_to_dict(input_string):\n",
    "    # Define the regex patterns to capture key-value pairs\n",
    "    string_pattern = r\"'(\\w+)': string_value: \\\"([^\\\"]*)\\\"\"\n",
    "    number_pattern = r\"'(\\w+)': number_value: (\\d+)\"\n",
    "    bool_pattern = r\"'(\\w+)': bool_value: (true|false)\"\n",
    "    \n",
    "    # Find all matches for strings, numbers, and booleans\n",
    "    string_matches = re.findall(string_pattern, input_string)\n",
    "    number_matches = re.findall(number_pattern, input_string)\n",
    "    bool_matches = re.findall(bool_pattern, input_string)\n",
    "\n",
    "    # Convert matches to a dictionary\n",
    "    result_dict = {key: value for key, value in string_matches}\n",
    "    result_dict.update({key: int(value) for key, value in number_matches})\n",
    "    result_dict.update({key: value == 'true' for key, value in bool_matches})\n",
    "\n",
    "    # Convert dictionary to JSON string\n",
    "    result_string = json.dumps(result_dict)\n",
    "    \n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "Raw response:\n",
      "[function_call {\n",
      "  name: \"send_mail\"\n",
      "  args {\n",
      "    fields {\n",
      "      key: \"to\"\n",
      "      value {\n",
      "        string_value: \"diogoazevedo15@gmail.com\"\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"subject\"\n",
      "      value {\n",
      "        string_value: \"hello\"\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"body\"\n",
      "      value {\n",
      "        string_value: \"hello\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "**********************************************************\n",
      "Function name: send_mail\n",
      "**********************************************************\n",
      "Raw Arguments:\n",
      "{'to': string_value: \"diogoazevedo15@gmail.com\"\n",
      ", 'subject': string_value: \"hello\"\n",
      ", 'body': string_value: \"hello\"\n",
      "}\n",
      "**********************************************************\n",
      "Response JSON: {\"to\": \"diogoazevedo15@gmail.com\", \"subject\": \"hello\", \"body\": \"hello\"}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for chunk in response:\n",
    "\n",
    "    # Print raw response\n",
    "    print('**********************************************************')   \n",
    "    print('Raw response:')\n",
    "    print(chunk.parts)\n",
    "    print('**********************************************************')   \n",
    "\n",
    "    # Print function name\n",
    "    name = chunk.parts[0].__dict__['_pb'].function_call.name\n",
    "    print(f'Function name: {name}')\n",
    "    print('**********************************************************')\n",
    "\n",
    "    # Print function arguments\n",
    "    args = chunk.parts[0].__dict__['_pb'].function_call.args.fields\n",
    "    args_str1 = str(chunk.parts[0].__dict__['_pb'].function_call.args.fields)\n",
    "    print('Raw Arguments:')\n",
    "    print(args_str1)\n",
    "    print('**********************************************************')\n",
    "\n",
    "    response_dict = {}\n",
    "    for i in args:\n",
    "        key = i\n",
    "        value = args[i].ListFields()[0][1]\n",
    "        \n",
    "        # Check if the value is a number\n",
    "        if isinstance(value, (int, float)):\n",
    "            # Convert to int if it has no decimal part, otherwise keep as float\n",
    "            value = int(value) if value == int(value) else float(value)\n",
    "        \n",
    "        response_dict[key] = value\n",
    "\n",
    "    response_json = json.dumps(response_dict)\n",
    "    print(f'Response JSON: {response_json}')\n",
    "        \n",
    "    # # Print function arguments\n",
    "    # args_json = json.dumps(args_str1)\n",
    "    # print('Test Arguments in JSON:')\n",
    "    # print(args_json)\n",
    "    # print('**********************************************************')\n",
    "\n",
    "    # # Print converted arguments\n",
    "    # opeiai_args = parse_string_to_dict(args_json)\n",
    "    # print('Converted Arguments:')\n",
    "    # print(opeiai_args)\n",
    "    # print('**********************************************************')\n",
    "\n",
    "    # Simulate Stream Call\n",
    "\n",
    "    # # First chunk that has function name\n",
    "    # id = str(uuid.uuid4())\n",
    "    # initial_chunk = ChatCompletionChunk(\n",
    "    #     id=id,\n",
    "    #     choices=[\n",
    "    #         Choice(\n",
    "    #             delta=ChoiceDelta(function_call=ChoiceDeltaFunctionCall(name=name)),\n",
    "    #             finish_reason=None,\n",
    "    #             index=0,\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     created=int(time.time()),\n",
    "    #     model='gemini-1.5-flash',\n",
    "    #     object=\"chat.completion.chunk\",\n",
    "    # ).model_dump()\n",
    "\n",
    "    # # Midel chunk with arguments\n",
    "    # middle_chunk = ChatCompletionChunk(\n",
    "    #     id=id,\n",
    "    #     choices=[\n",
    "    #         Choice(\n",
    "    #             delta=ChoiceDelta(function_call=ChoiceDeltaFunctionCall(arguments=opeiai_args)),\n",
    "    #             finish_reason=None,\n",
    "    #             index=0,\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     created=int(time.time()),\n",
    "    #     model='gemini-1.5-flash',\n",
    "    #     object=\"chat.completion.chunk\",\n",
    "    # ).model_dump()\n",
    "\n",
    "    # print(\"_\" * 80)\n",
    "\n",
    "    # print(initial_chunk)\n",
    "    # print('**********************************************************')\n",
    "    # print(middle_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normal call VS function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash', tools=[oai_functions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(prompts[0], stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('Hello Brother', stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk is normal\n",
      "Chunk is normal\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    if chunk.parts[0].function_call:\n",
    "        print('Chunk is function')\n",
    "    elif chunk.parts[0].text:\n",
    "        print('Chunk is normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LLMstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_functions = [\n",
    "    {\n",
    "        \"name\": \"multiply\",\n",
    "        \"description\": \"Returns the product of two numbers.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number.\"\n",
    "                },\n",
    "                \"b\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"turn_lights\",\n",
    "        \"description\": \"Turns the lights on or off.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"state\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"The desired state of the lights.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"state\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_mail\",\n",
    "        \"description\": \"Sends an email to a specified address.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"to\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The recipient's email address.\"\n",
    "                },\n",
    "                \"subject\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The subject of the email.\"\n",
    "                },\n",
    "                \"body\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The body of the email.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"to\", \"subject\", \"body\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "prompts = ['Whats 0.9 times 10?',\n",
    "           'Send an email to diogoazevedo15@gmail.com. Send hello in the body and make the subject be hello as well.',\n",
    "           'Turn on the light.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:51953 Running LLMstudio Tracking on http://localhost:51954 \n",
      "\n",
      "providers.py - request api_key=None model='gpt-3.5-turbo' chat_input='Turn on the light.' parameters=OpenAIParameters(temperature=None, max_tokens=None, top_p=None, frequency_penalty=None, presence_penalty=None) is_stream=False has_end_token=False functions=[{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'The first number.'}, 'b': {'type': 'number', 'description': 'The second number.'}}, 'required': ['a', 'b']}}, {'name': 'turn_lights', 'description': 'Turns the lights on or off.', 'parameters': {'type': 'object', 'properties': {'state': {'type': 'boolean', 'description': 'The desired state of the lights.'}}, 'required': ['state']}}, {'name': 'send_mail', 'description': 'Sends an email to a specified address.', 'parameters': {'type': 'object', 'properties': {'to': {'type': 'string', 'description': \"The recipient's email address.\"}, 'subject': {'type': 'string', 'description': 'The subject of the email.'}, 'body': {'type': 'string', 'description': 'The body of the email.'}}, 'required': ['to', 'subject', 'body']}}] session_id=None retries=0 response_format=None\n",
      "providers.py - request api_key=None model='gemini-1.5-flash' chat_input='Turn on the light.' parameters=VertexAIParameters(top_p=None, top_k=None, temperature=None, max_output_tokens=None, frequency_penalty=None, presence_penalty=None) is_stream=False has_end_token=False functions=[[{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'The first number.'}, 'b': {'type': 'number', 'description': 'The second number.'}}, 'required': ['a', 'b']}}, {'name': 'turn_lights', 'description': 'Turns the lights on or off.', 'parameters': {'type': 'object', 'properties': {'state': {'type': 'boolean', 'description': 'The desired state of the lights.'}}, 'required': ['state']}}, {'name': 'send_mail', 'description': 'Sends an email to a specified address.', 'parameters': {'type': 'object', 'properties': {'to': {'type': 'string', 'description': \"The recipient's email address.\"}, 'subject': {'type': 'string', 'description': 'The subject of the email.'}, 'body': {'type': 'string', 'description': 'The body of the email.'}}, 'required': ['to', 'subject', 'body']}}]] session_id=None retries=0\n",
      "vertex.py - request: Turn on the light.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('openai/gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='49b0597e-dff0-45c4-a129-45972b4294ed', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"state\":true}', name='turn_lights'), tool_calls=None))], created=1721646694, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Turn on the light.', chat_output='{\"state\":true}', context=[{'role': 'user', 'content': 'Turn on the light.'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1721646694.295469, parameters={'temperature': None, 'max_tokens': None, 'top_p': None, 'frequency_penalty': None, 'presence_penalty': None}, metrics={'input_tokens': 5, 'output_tokens': 5, 'total_tokens': 10, 'cost_usd': 9.999999999999999e-06, 'latency_s': 1.3316190242767334, 'time_to_first_token_s': 1.3017609119415283, 'inter_token_latency_s': 0.004197160402933757, 'tokens_per_second': 5.256758781891118})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat(prompts[2], functions = oai_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:52056 Running LLMstudio Tracking on http://localhost:52057 \n",
      "\n",
      "providers.py - request api_key=None model='gemini-1.5-flash' chat_input='Turn on the light.' parameters=VertexAIParameters(top_p=None, top_k=None, temperature=None, max_output_tokens=None, frequency_penalty=None, presence_penalty=None) is_stream=False has_end_token=False functions=[{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'The first number.'}, 'b': {'type': 'number', 'description': 'The second number.'}}, 'required': ['a', 'b']}}, {'name': 'turn_lights', 'description': 'Turns the lights on or off.', 'parameters': {'type': 'object', 'properties': {'state': {'type': 'boolean', 'description': 'The desired state of the lights.'}}, 'required': ['state']}}, {'name': 'send_mail', 'description': 'Sends an email to a specified address.', 'parameters': {'type': 'object', 'properties': {'to': {'type': 'string', 'description': \"The recipient's email address.\"}, 'subject': {'type': 'string', 'description': 'The subject of the email.'}, 'body': {'type': 'string', 'description': 'The body of the email.'}}, 'required': ['to', 'subject', 'body']}}] session_id=None retries=0\n",
      "vertex.py - request: Turn on the light.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('vertexai/gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='28d36907-e773-4610-a321-9537194ecbe2', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"state\":1}', name='turn_lights'), tool_calls=None))], created=1721646893, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Turn on the light.', chat_output='{\"state\":1}', context=[{'role': 'user', 'content': 'Turn on the light.'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1721646893.01521, parameters={'top_p': None, 'top_k': None, 'temperature': None, 'max_output_tokens': None, 'frequency_penalty': None, 'presence_penalty': None}, metrics={'input_tokens': 5, 'output_tokens': 5, 'total_tokens': 10, 'cost_usd': 7e-06, 'latency_s': 1.416579008102417, 'time_to_first_token_s': 1.412843942642212, 'inter_token_latency_s': 1.1563301086425781e-05, 'tokens_per_second': 2.1177780998030316})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat(prompts[2], functions = oai_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='4a69724e-0bd2-4067-b448-2f53a4b48b8f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Yo! What's up? \\n\", role='assistant', function_call=None, tool_calls=None))], created=1721644628, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Yo', chat_output=\"Yo! What's up? \\n\", context=[{'role': 'user', 'content': 'Yo'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1721644628.0709069, parameters={'top_p': None, 'top_k': None, 'temperature': None, 'max_output_tokens': None, 'frequency_penalty': None, 'presence_penalty': None}, metrics={'input_tokens': 1, 'output_tokens': 7, 'total_tokens': 8, 'cost_usd': 7.699999999999999e-06, 'latency_s': 1.389686107635498, 'time_to_first_token_s': 1.3498189449310303, 'inter_token_latency_s': 0.013272762298583984, 'tokens_per_second': 2.8783478355452936})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Yo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM('openai/gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='e8858197-06d1-49ff-a2aa-0f82eab19c27', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1721312880, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Hello', chat_output='Hello! How can I assist you today?', context=[{'role': 'user', 'content': 'Hello'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1721312881.270113, parameters={'temperature': None, 'max_tokens': None, 'top_p': None, 'frequency_penalty': None, 'presence_penalty': None}, metrics={'input_tokens': 1, 'output_tokens': 9, 'total_tokens': 10, 'cost_usd': 1.4e-05, 'latency_s': 1.0538489818572998, 'time_to_first_token_s': 0.9661450386047363, 'inter_token_latency_s': 0.008752202987670899, 'tokens_per_second': 10.437928194050762})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messege = [{'role': 'system', 'content': 'You are a helpful AI assistant.'}, {'role': 'user', 'content': 'When does my train depart? My ticket number is 1234'}, {'role': 'assistant', 'content': None, 'function_call': {'arguments': '{\"ticket_number\":\"1234\"}', 'name': 'get_departure'}}, {'role': 'function', 'content': '12:00 AM', 'name': 'get_departure'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Could not recognize the intended type of the `dict`. A `Content` should have a 'parts' key. A `Part` should have a 'inline_data' or a 'text' key. A `Blob` should have 'mime_type' and 'data' keys. Got keys: ['role', 'content']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessege\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/generativeai/generative_models.py:240\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_content\u001b[39m(\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    177\u001b[0m     contents: content_types\u001b[38;5;241m.\u001b[39mContentsType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m     request_options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    185\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse:\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A multipurpose function to generate responses from the model.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    This `GenerativeModel.generate_content` method can handle multimodal input, and multi-turn\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m        request_options: Options for the request.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_default_generative_client()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/generativeai/generative_models.py:146\u001b[0m, in \u001b[0;36mGenerativeModel._prepare_request\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     tool_config \u001b[38;5;241m=\u001b[39m content_types\u001b[38;5;241m.\u001b[39mto_tool_config(tool_config)\n\u001b[0;32m--> 146\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m generation_types\u001b[38;5;241m.\u001b[39mto_generation_config_dict(generation_config)\n\u001b[1;32m    149\u001b[0m merged_gc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generation_config\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/generativeai/types/content_types.py:307\u001b[0m, in \u001b[0;36mto_contents\u001b[0;34m(contents)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(contents, Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(contents, (\u001b[38;5;28mstr\u001b[39m, Mapping)):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# strict_to_content so [[parts], [parts]] doesn't assume roles.\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m         contents \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mstrict_to_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m contents\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;66;03m# If you get a TypeError here it's probably because that was a list\u001b[39;00m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;66;03m# of parts, not a list of contents, so fall back to `to_content`.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/generativeai/types/content_types.py:307\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(contents, Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(contents, (\u001b[38;5;28mstr\u001b[39m, Mapping)):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# strict_to_content so [[parts], [parts]] doesn't assume roles.\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m         contents \u001b[38;5;241m=\u001b[39m [\u001b[43mstrict_to_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m contents]\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m contents\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;66;03m# If you get a TypeError here it's probably because that was a list\u001b[39;00m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;66;03m# of parts, not a list of contents, so fall back to `to_content`.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/generativeai/types/content_types.py:285\u001b[0m, in \u001b[0;36mstrict_to_content\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstrict_to_content\u001b[39m(content: StrictContentType):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, Mapping):\n\u001b[0;32m--> 285\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, glm\u001b[38;5;241m.\u001b[39mContent):\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/generativeai/types/content_types.py:137\u001b[0m, in \u001b[0;36m_convert_dict\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m glm\u001b[38;5;241m.\u001b[39mBlob(blob)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not recognize the intended type of the `dict`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA `Content` should have a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA `Part` should have a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline_data\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA `Blob` should have \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmime_type\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(d\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Could not recognize the intended type of the `dict`. A `Content` should have a 'parts' key. A `Part` should have a 'inline_data' or a 'text' key. A `Blob` should have 'mime_type' and 'data' keys. Got keys: ['role', 'content']\""
     ]
    }
   ],
   "source": [
    "model.generate_content(messege)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM('vertexai/gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:50001 Running LLMstudio Tracking on http://localhost:50002 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='17e24946-24c9-4203-8179-89359a1d3ceb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I help you today? \\n', role='assistant', function_call=None, tool_calls=None))], created=1721733998, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Hello', chat_output='Hello! How can I help you today? \\n', context=[{'role': 'user', 'content': 'Hello'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1721733998.622133, parameters={'top_p': None, 'top_k': None, 'temperature': None, 'max_output_tokens': None, 'frequency_penalty': None, 'presence_penalty': None}, metrics={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'cost_usd': 1.085e-05, 'latency_s': 1.4075579643249512, 'time_to_first_token_s': 1.3842499256134033, 'inter_token_latency_s': 0.005758364995320638, 'tokens_per_second': 2.8418012624569635})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
