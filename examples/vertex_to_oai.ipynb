{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator_gemni = {'function_declarations': [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} },\n",
    "      {'name': 'send_mail',\n",
    "       'description': 'Sends an email to a specified address.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'to': {'type_': 'STRING'},\n",
    "         'subject': {'type_': 'STRING'},\n",
    "         'body': {'type_': 'STRING'} },\n",
    "       'required': ['to', 'subject', 'body']} },\n",
    "      {'name': 'turn_lights',\n",
    "       'description': 'Turns the lights on or off.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'state': {'type_': 'BOOLEAN'} },\n",
    "       'required': ['state']} }\n",
    "    ]\n",
    "}\n",
    "\n",
    "calculator_oai = [\n",
    "    {\n",
    "        \"name\": \"multiply\",\n",
    "        \"description\": \"Returns the product of two numbers.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number.\"\n",
    "                },\n",
    "                \"b\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"turn_lights\",\n",
    "        \"description\": \"Turns the lights on or off.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"state\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"The desired state of the lights.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"state\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_mail\",\n",
    "        \"description\": \"Sends an email to a specified address.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"to\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The recipient's email address.\"\n",
    "                },\n",
    "                \"subject\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The subject of the email.\"\n",
    "                },\n",
    "                \"body\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The body of the email.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"to\", \"subject\", \"body\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time  # for measuring time duration of API calls\n",
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='', name='send_mail'), role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='{\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='to', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='example', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='@gmail', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.com', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\",\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='subject', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='Regarding', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' Important', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' Information', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\",\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='body', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='Hello', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=',\\\\', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\\\\n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='I', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' hope', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' this', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' email', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' finds', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' you', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' well', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' I', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' am', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' writing', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' to', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' share', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' some', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' important', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' information', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' with', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' you', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' Please', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' let', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' me', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' know', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' if', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' you', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' have', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' any', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' questions', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.\\\\', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\\\\n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='Best', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' regards', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=',\\\\', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='n', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='[', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='Your', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=' Name', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments=']', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\"}', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eWCUASYHXsJt43kgKfxVA3hTzj', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1721229280, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "# a ChatCompletion request\n",
    "openai_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write an email to example@gmail.com. Make up the rest.\"}\n",
    "    ],\n",
    "    functions = calculator_oai,\n",
    "    function_call={\"name\": \"send_mail\"},\n",
    "    temperature=0,\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in openai_response:\n",
    "    print(chunk)\n",
    "    # print(chunk.choices[0].delta.content)\n",
    "    print(\"****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9m0eYkLEDEreH1et3AnYJHnlsitU4', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"state\":true}', name='turn_lights'), tool_calls=None))], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=154, total_tokens=169))\n"
     ]
    }
   ],
   "source": [
    "# a ChatCompletion request\n",
    "openai_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Turn the lights on\"}\n",
    "    ],\n",
    "    functions = calculator_oai,\n",
    "    temperature=0,\n",
    "    stream=False  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatCompletion(id='chatcmpl-9m0eYkLEDEreH1et3AnYJHnlsitU4', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"state\":true}', name='turn_lights'), tool_calls=None))], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=154, total_tokens=169))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n",
      "ChatCompletionChunk(id='chatcmpl-9m0eYa3mXTUANQl5NHqzXMqQmOxbJ', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1721229282, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "# a ChatCompletion request\n",
    "openai_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in openai_response:\n",
    "    print(chunk)\n",
    "    # print(chunk.choices[0].delta.content)\n",
    "    print(\"****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertexai Function + Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash', tools=[calculator_gemni])\n",
    "response = model.generate_content('whats 9 times 10', stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[function_call {\n",
      "  name: \"multiply\"\n",
      "  args {\n",
      "    fields {\n",
      "      key: \"b\"\n",
      "      value {\n",
      "        number_value: 10\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"a\"\n",
      "      value {\n",
      "        number_value: 9\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "multiply\n",
      "{'a': number_value: 9\n",
      ", 'b': number_value: 10\n",
      "}\n",
      "{\"a\": 9, \"b\": 10}\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from openai.types.chat import ChatCompletionChunk\n",
    "from openai.types.chat.chat_completion_chunk import Choice, ChoiceDelta\n",
    "from openai.types.chat.chat_completion_chunk import ChoiceDeltaFunctionCall\n",
    "# from openai.types.chat import FunctionCall\n",
    "import uuid\n",
    "import re\n",
    "import json\n",
    "\n",
    "def convert_to_json(custom_str):\n",
    "    # Remove whitespace and newlines\n",
    "    custom_str = custom_str.replace('\\n', '').replace(' ', '')\n",
    "    \n",
    "    # Use regex to find key-value pairs\n",
    "    matches = re.findall(r\"'(\\w+)':number_value:(\\d+)\", custom_str)\n",
    "    \n",
    "    # Convert matches to a dictionary\n",
    "    result_dict = {key: int(value) for key, value in matches}\n",
    "    \n",
    "    # Convert dictionary to JSON string\n",
    "    json_str = json.dumps(result_dict)\n",
    "    return json_str\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.parts)\n",
    "    # print(chunk.parts[0].function_call.name)\n",
    "\n",
    "    name = chunk.parts[0].__dict__['_pb'].function_call.name\n",
    "    print(name)\n",
    "    args = chunk.parts[0].__dict__['_pb'].function_call.args.fields\n",
    "    args_str1 = str(chunk.parts[0].__dict__['_pb'].function_call.args.fields)\n",
    "    print(args_str1)\n",
    "    arg_str2 = convert_to_json(args_str1)\n",
    "    print(arg_str2)\n",
    "    # for i in args:\n",
    "    #     print(chunk.parts[0].__dict__['_pb'].function_call.args.fields[i])\n",
    "    # print(args)\n",
    "    \n",
    "\n",
    "\n",
    "    # Construct string\n",
    "    \n",
    "\n",
    "    # ChatCompletionChunk(\n",
    "    #     id=str(uuid.uuid4()),\n",
    "    #     choices=[\n",
    "    #         Choice(\n",
    "    #             delta=ChoiceDelta(function_call=FunctionCall(arguments=arg_str2, name=name)),\n",
    "    #             finish_reason=None,\n",
    "    #             index=0,\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     created=int(time.time()),\n",
    "    #     model='gemini-1.5-flash',\n",
    "    #     object=\"chat.completion.chunk\",\n",
    "    # ).model_dump()\n",
    "    \n",
    "    print(\"_\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def convert_to_json(custom_str):\n",
    "    # Remove whitespace and newlines\n",
    "    custom_str = custom_str.replace('\\n', '').replace(' ', '')\n",
    "    \n",
    "    # Use regex to find key-value pairs\n",
    "    matches = re.findall(r\"'(\\w+)':number_value:(\\d+)\", custom_str)\n",
    "    \n",
    "    # Convert matches to a dictionary\n",
    "    result_dict = {key: int(value) for key, value in matches}\n",
    "    \n",
    "    # Convert dictionary to JSON string\n",
    "    json_str = json.dumps(result_dict)\n",
    "    return json_str\n",
    "\n",
    "# Example usage\n",
    "custom_str = \"{'a': number_value: 9, 'b': number_value: 10}\"\n",
    "json_str = convert_to_json(custom_str)\n",
    "print(json_str)  # Output: {\"a\": 9, \"b\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. OpenAI format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function + Streaming\n",
    "\n",
    "****************\n",
    "**First chunk**\n",
    "\n",
    "ChatCompletionChunk(id='chatcmpl-9ly0H0yO7PorUjPyBKUXW04c6ZpQ7', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='', name='send_mail'), role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721219097, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
    "****************\n",
    "\n",
    "****************\n",
    "**Middle Chunk**\n",
    "\n",
    "ChatCompletionChunk(id='chatcmpl-9ly0H0yO7PorUjPyBKUXW04c6ZpQ7', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='{\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721219097, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
    "****************\n",
    "\n",
    "****************\n",
    "**Final Chunk**\n",
    "\n",
    "ChatCompletionChunk(id='chatcmpl-9ly0H0yO7PorUjPyBKUXW04c6ZpQ7', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1721219097, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
    "****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function\n",
    "\n",
    "****************\n",
    "ChatCompletion(id='chatcmpl-9lvzlofzJbCBtghCboLCxQLeGi86J', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"a\":6,\"b\":7}', name='multiply'), tool_calls=None))], created=1721211377, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=84, total_tokens=93))\n",
    "****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gemini format\n",
    "It's always in the same format. Stream or not stream, it does not make a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[function_call {\n",
    "  name: \"multiply\"\n",
    "  args {\n",
    "    fields {\n",
    "      key: \"b\"\n",
    "      value {\n",
    "        number_value: 10\n",
    "      }\n",
    "    }\n",
    "    fields {\n",
    "      key: \"a\"\n",
    "      value {\n",
    "        number_value: 9\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functions\n",
    "1. Multiply a by b\n",
    "2. Send email with to, subject and body\n",
    "3. Turn on and of the light with a boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_functions = {'function_declarations': [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} },\n",
    "      {'name': 'send_mail',\n",
    "       'description': 'Sends an email to a specified address.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'to': {'type_': 'STRING'},\n",
    "         'subject': {'type_': 'STRING'},\n",
    "         'body': {'type_': 'STRING'} },\n",
    "       'required': ['to', 'subject', 'body']} },\n",
    "      {'name': 'turn_lights',\n",
    "       'description': 'Turns the lights on or off.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'state': {'type_': 'BOOLEAN'} },\n",
    "       'required': ['state']} }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_functions = [\n",
    "    {\n",
    "        \"name\": \"multiply\",\n",
    "        \"description\": \"Returns the product of two numbers.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number.\"\n",
    "                },\n",
    "                \"b\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"turn_lights\",\n",
    "        \"description\": \"Turns the lights on or off.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"state\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"The desired state of the lights.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"state\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_mail\",\n",
    "        \"description\": \"Sends an email to a specified address.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"to\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The recipient's email address.\"\n",
    "                },\n",
    "                \"subject\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The subject of the email.\"\n",
    "                },\n",
    "                \"body\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The body of the email.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"to\", \"subject\", \"body\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vertex to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Make Chat call with functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Setup model with tools\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel('gemini-1.5-flash', tools=[oai_functions])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "prompts = ['Whats 0.9 times 10?',\n",
    "           'Send an email to diogoazevedo15@gmail.com. Send hello in the body and make the subject be hello as well.',\n",
    "           'Turn of the light.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get response\n",
    "response = model.generate_content(prompts[1], stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Convert chatcall from VertexAI to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletionChunk, ChatCompletion\n",
    "from openai.types.chat.chat_completion_chunk import Choice, ChoiceDelta\n",
    "from openai.types.chat.chat_completion_chunk import ChoiceDeltaFunctionCall\n",
    "# from openai.types.chat import FunctionCall\n",
    "from openai.types.chat.chat_completion_message import ChatCompletionMessage, FunctionCall\n",
    "import uuid\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def parse_string_to_dict(input_string):\n",
    "    # Define the regex patterns to capture key-value pairs\n",
    "    string_pattern = r\"'(\\w+)': string_value: \\\"([^\\\"]*)\\\"\"\n",
    "    number_pattern = r\"'(\\w+)': number_value: (\\d+)\"\n",
    "    bool_pattern = r\"'(\\w+)': bool_value: (true|false)\"\n",
    "    \n",
    "    # Find all matches for strings, numbers, and booleans\n",
    "    string_matches = re.findall(string_pattern, input_string)\n",
    "    number_matches = re.findall(number_pattern, input_string)\n",
    "    bool_matches = re.findall(bool_pattern, input_string)\n",
    "\n",
    "    # Convert matches to a dictionary\n",
    "    result_dict = {key: value for key, value in string_matches}\n",
    "    result_dict.update({key: int(value) for key, value in number_matches})\n",
    "    result_dict.update({key: value == 'true' for key, value in bool_matches})\n",
    "\n",
    "    # Convert dictionary to JSON string\n",
    "    result_string = json.dumps(result_dict)\n",
    "    \n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "Raw response:\n",
      "[function_call {\n",
      "  name: \"send_mail\"\n",
      "  args {\n",
      "    fields {\n",
      "      key: \"to\"\n",
      "      value {\n",
      "        string_value: \"diogoazevedo15@gmail.com\"\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"subject\"\n",
      "      value {\n",
      "        string_value: \"hello\"\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"body\"\n",
      "      value {\n",
      "        string_value: \"hello\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "**********************************************************\n",
      "Function name: send_mail\n",
      "**********************************************************\n",
      "Raw Arguments:\n",
      "{'to': string_value: \"diogoazevedo15@gmail.com\"\n",
      ", 'subject': string_value: \"hello\"\n",
      ", 'body': string_value: \"hello\"\n",
      "}\n",
      "**********************************************************\n",
      "Response JSON: {\"to\": \"diogoazevedo15@gmail.com\", \"subject\": \"hello\", \"body\": \"hello\"}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for chunk in response:\n",
    "\n",
    "    # Print raw response\n",
    "    print('**********************************************************')   \n",
    "    print('Raw response:')\n",
    "    print(chunk.parts)\n",
    "    print('**********************************************************')   \n",
    "\n",
    "    # Print function name\n",
    "    name = chunk.parts[0].__dict__['_pb'].function_call.name\n",
    "    print(f'Function name: {name}')\n",
    "    print('**********************************************************')\n",
    "\n",
    "    # Print function arguments\n",
    "    args = chunk.parts[0].__dict__['_pb'].function_call.args.fields\n",
    "    args_str1 = str(chunk.parts[0].__dict__['_pb'].function_call.args.fields)\n",
    "    print('Raw Arguments:')\n",
    "    print(args_str1)\n",
    "    print('**********************************************************')\n",
    "\n",
    "    response_dict = {}\n",
    "    for i in args:\n",
    "        key = i\n",
    "        value = args[i].ListFields()[0][1]\n",
    "        \n",
    "        # Check if the value is a number\n",
    "        if isinstance(value, (int, float)):\n",
    "            # Convert to int if it has no decimal part, otherwise keep as float\n",
    "            value = int(value) if value == int(value) else float(value)\n",
    "        \n",
    "        response_dict[key] = value\n",
    "\n",
    "    response_json = json.dumps(response_dict)\n",
    "    print(f'Response JSON: {response_json}')\n",
    "        \n",
    "    # # Print function arguments\n",
    "    # args_json = json.dumps(args_str1)\n",
    "    # print('Test Arguments in JSON:')\n",
    "    # print(args_json)\n",
    "    # print('**********************************************************')\n",
    "\n",
    "    # # Print converted arguments\n",
    "    # opeiai_args = parse_string_to_dict(args_json)\n",
    "    # print('Converted Arguments:')\n",
    "    # print(opeiai_args)\n",
    "    # print('**********************************************************')\n",
    "\n",
    "    # Simulate Stream Call\n",
    "\n",
    "    # # First chunk that has function name\n",
    "    # id = str(uuid.uuid4())\n",
    "    # initial_chunk = ChatCompletionChunk(\n",
    "    #     id=id,\n",
    "    #     choices=[\n",
    "    #         Choice(\n",
    "    #             delta=ChoiceDelta(function_call=ChoiceDeltaFunctionCall(name=name)),\n",
    "    #             finish_reason=None,\n",
    "    #             index=0,\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     created=int(time.time()),\n",
    "    #     model='gemini-1.5-flash',\n",
    "    #     object=\"chat.completion.chunk\",\n",
    "    # ).model_dump()\n",
    "\n",
    "    # # Midel chunk with arguments\n",
    "    # middle_chunk = ChatCompletionChunk(\n",
    "    #     id=id,\n",
    "    #     choices=[\n",
    "    #         Choice(\n",
    "    #             delta=ChoiceDelta(function_call=ChoiceDeltaFunctionCall(arguments=opeiai_args)),\n",
    "    #             finish_reason=None,\n",
    "    #             index=0,\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     created=int(time.time()),\n",
    "    #     model='gemini-1.5-flash',\n",
    "    #     object=\"chat.completion.chunk\",\n",
    "    # ).model_dump()\n",
    "\n",
    "    # print(\"_\" * 80)\n",
    "\n",
    "    # print(initial_chunk)\n",
    "    # print('**********************************************************')\n",
    "    # print(middle_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normal call VS function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash', tools=[oai_functions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(prompts[0], stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('Hello Brother', stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk is normal\n",
      "Chunk is normal\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    if chunk.parts[0].function_call:\n",
    "        print('Chunk is function')\n",
    "    elif chunk.parts[0].text:\n",
    "        print('Chunk is normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LLMstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_functions = [\n",
    "    {\n",
    "        \"name\": \"multiply\",\n",
    "        \"description\": \"Returns the product of two numbers.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"a\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number.\"\n",
    "                },\n",
    "                \"b\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"a\", \"b\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"turn_lights\",\n",
    "        \"description\": \"Turns the lights on or off.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"state\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"The desired state of the lights.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"state\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_mail\",\n",
    "        \"description\": \"Sends an email to a specified address.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"to\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The recipient's email address.\"\n",
    "                },\n",
    "                \"subject\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The subject of the email.\"\n",
    "                },\n",
    "                \"body\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The body of the email.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"to\", \"subject\", \"body\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "prompts = ['Whats 0.9 times 10?',\n",
    "           'Send an email to diogoazevedo15@gmail.com. Send hello in the body and make the subject be hello as well.',\n",
    "           'Turn of the light.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Tracking on http://localhost:50002 Running LLMstudio Engine on http://localhost:50001 \n",
      "\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='', name='send_mail'), role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='{\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='body', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='hello', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\",\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='subject', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='hello', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\",\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='to', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\":\"', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='di', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='ogo', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='aze', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='ved', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='o', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='15', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='@gmail', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='.com', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=ChoiceDeltaFunctionCall(arguments='\"}', name=None), role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9mImEMSGaR4OYZgBAjGAJTSyVR1hG', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='function_call', index=0, logprobs=None)], created=1721298950, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "provider.py - before function call name\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('openai/gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions in init.py sdk:  [{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'The first number.'}, 'b': {'type': 'number', 'description': 'The second number.'}}, 'required': ['a', 'b']}}, {'name': 'turn_lights', 'description': 'Turns the lights on or off.', 'parameters': {'type': 'object', 'properties': {'state': {'type': 'boolean', 'description': 'The desired state of the lights.'}}, 'required': ['state']}}, {'name': 'send_mail', 'description': 'Sends an email to a specified address.', 'parameters': {'type': 'object', 'properties': {'to': {'type': 'string', 'description': \"The recipient's email address.\"}, 'subject': {'type': 'string', 'description': 'The subject of the email.'}, 'body': {'type': 'string', 'description': 'The body of the email.'}}, 'required': ['to', 'subject', 'body']}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='2b2ad021-4cca-4726-83d1-118b6966908d', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"body\":\"hello\",\"subject\":\"hello\",\"to\":\"diogoazevedo15@gmail.com\"}', name='send_mail'), tool_calls=None))], created=1721298950, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Send an email to diogoazevedo15@gmail.com. Send hello in the body and make the subject be hello as well.', chat_output='{\"body\":\"hello\",\"subject\":\"hello\",\"to\":\"diogoazevedo15@gmail.com\"}', context=[{'role': 'user', 'content': 'Send an email to diogoazevedo15@gmail.com. Send hello in the body and make the subject be hello as well.'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1721298950.770405, parameters={'temperature': None, 'max_tokens': None, 'top_p': None, 'frequency_penalty': None, 'presence_penalty': None}, metrics={'input_tokens': 27, 'output_tokens': 20, 'total_tokens': 47, 'cost_usd': 4.35e-05, 'latency_s': 1.292431116104126, 'time_to_first_token_s': 1.0808839797973633, 'inter_token_latency_s': 0.009799480438232422, 'tokens_per_second': 17.022183794457288})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat(prompts[1], functions = oai_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:50001 Running LLMstudio Tracking on http://localhost:50002 \n",
      "\n",
      "vertex.py - functions detected\n",
      "vertex.py - parse response\n",
      "vertex.py - parse response - function call detected\n",
      "vertex.py - args_json - \"{'a': number_value: 0.9\\n, 'b': number_value: 10\\n}\"\n",
      "An error occurred: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('vertexai/gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions in init.py sdk:  [[{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'The first number.'}, 'b': {'type': 'number', 'description': 'The second number.'}}, 'required': ['a', 'b']}}, {'name': 'turn_lights', 'description': 'Turns the lights on or off.', 'parameters': {'type': 'object', 'properties': {'state': {'type': 'boolean', 'description': 'The desired state of the lights.'}}, 'required': ['state']}}, {'name': 'send_mail', 'description': 'Sends an email to a specified address.', 'parameters': {'type': 'object', 'properties': {'to': {'type': 'string', 'description': \"The recipient's email address.\"}, 'subject': {'type': 'string', 'description': 'The subject of the email.'}, 'body': {'type': 'string', 'description': 'The body of the email.'}}, 'required': ['to', 'subject', 'body']}}]]\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: http://localhost:50001/api/engine/chat/vertexai",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43moai_functions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/__init__.py:63\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, input, is_stream, retries, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunctions in init.py sdk: \u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     34\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_HOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_PORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/engine/chat/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: http://localhost:50001/api/engine/chat/vertexai"
     ]
    }
   ],
   "source": [
    "llm.chat(prompts[0], functions = [oai_functions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def parse_string_to_dict(args):\n",
    "        response_dict = {}\n",
    "        for i in args:\n",
    "                key = i\n",
    "                value = args[i].ListFields()[0][1]\n",
    "                \n",
    "                # Check if the value is a number\n",
    "                if isinstance(value, (int, float)):\n",
    "                        # Convert to int if it has no decimal part, otherwise keep as float\n",
    "                        value = int(value) if value == int(value) else float(value)\n",
    "                response_dict[key] = value\n",
    "\n",
    "        response_json = json.dumps(response_dict)\n",
    "        return response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertex.py - parse_string - string_matches - []\n",
      "vertex.py - parse_string - result_dict - {}\n",
      "vertex.py - parse_string - result_dict - {'a': 0, 'b': 10}\n",
      "vertex.py - parse_string - result_dict - {'a': 0, 'b': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"a\": 0, \"b\": 10}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = \"{'a': number_value: 0.9\\n, 'b': number_value: 10\\n}\"\n",
    "parse_string_to_dict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'a'\", ' number_value', \" 0.9\\n, 'b'\", ' number_value', ' 10\\n}']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_string = args.split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in split_string:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
