{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertex AI function tests\n",
    "\n",
    "NOTEBOOK TLDR:\n",
    "1. The Google function calling by default uses functions as inputs. We can not send functions trought the api...\n",
    "2. I found how this works and they turn the function into a **generativelanguage_v1beta.types.content.Tool.**\n",
    "3. We can pass either the function or the **google.ai.generativelanguage_v1beta.types.content.Tool** as inputs for the tools list.\n",
    "4. To implement this feature we need to find out when the client is using VertexAI (sdk side), and parse the functions into **google.ai.generativelanguage_v1beta.types.content.Tool** before sending it to the engine side.\n",
    "\n",
    "Example: https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python#get-and-secure-api-key\n",
    "\n",
    "Proto Format: https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python#generate-function-call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define function\n",
    "Here we define the function we are going to give the model. Don't forget to give typings to the the atributes, otherwise the function will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google supported atributes. Dont run this cell\n",
    "# AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_light_values(brightness: int, color_temp: int):\n",
    "    \"\"\"Set the brightness and color temperature of a room light. (mock API).\n",
    "\n",
    "    Args:\n",
    "        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n",
    "        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the set brightness and color temperature.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"brightness\": brightness,\n",
    "        \"colorTemperature\": color_temp\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define model with tools\n",
    "Here I want to show that the model works in both this cases:\n",
    "1. Giving a function as a tool.\n",
    "2. Giving a proto version of the function as a tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Function as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
    "                              tools=[set_light_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=glm.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"set_light_values\",\n",
       "                  \"args\": {\n",
       "                    \"brightness\": 50.0,\n",
       "                    \"color_temp\": \"warm\"\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": 1,\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": 9,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 8,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 7,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 10,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            }\n",
       "          ],\n",
       "          \"token_count\": 0,\n",
       "          \"grounding_attributions\": []\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 149,\n",
       "        \"candidates_token_count\": 25,\n",
       "        \"total_token_count\": 174\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.generate_content('Dim the lights so the room feels cozy and warm.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Proto function as input\n",
    "A description of the function that allows it to be transmitable over an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_declarations {\n",
      "  name: \"set_light_values\"\n",
      "  description: \"Set the brightness and color temperature of a room light. (mock API).\\n\\n    Args:\\n        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\\n        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\\n\\n    Returns:\\n        A dictionary containing the set brightness and color temperature.\\n    \"\n",
      "  parameters {\n",
      "    type_: OBJECT\n",
      "    properties {\n",
      "      key: \"color_temp\"\n",
      "      value {\n",
      "        type_: INTEGER\n",
      "      }\n",
      "    }\n",
      "    properties {\n",
      "      key: \"brightness\"\n",
      "      value {\n",
      "        type_: INTEGER\n",
      "      }\n",
      "    }\n",
      "    required: \"brightness\"\n",
      "    required: \"color_temp\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "proto_function = model_1._tools.to_proto()[0]\n",
    "print(proto_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
    "                              tools=[proto_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=glm.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"set_light_values\",\n",
       "                  \"args\": {\n",
       "                    \"brightness\": 50.0,\n",
       "                    \"color_temp\": \"warm\"\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": 1,\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": 9,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 7,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 10,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 8,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            }\n",
       "          ],\n",
       "          \"token_count\": 0,\n",
       "          \"grounding_attributions\": []\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 149,\n",
       "        \"candidates_token_count\": 25,\n",
       "        \"total_token_count\": 174\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.generate_content('Dim the lights so the room feels cozy and warm.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Calculator Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = {'function_declarations': [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} }]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
    "                              tools=[calculator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=glm.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"multiply\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 9.0,\n",
       "                    \"b\": 10.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": 1,\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": 10,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 7,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 9,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 8,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            }\n",
       "          ],\n",
       "          \"token_count\": 0,\n",
       "          \"grounding_attributions\": []\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 57,\n",
       "        \"candidates_token_count\": 19,\n",
       "        \"total_token_count\": 76\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.generate_content('What is 9 times 10?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "1. We need to understand when the user is sending a VertexAI request.\n",
    "2. On the client side, we convert the function passed, to the proto format.\n",
    "3. We make the call to the engine and get a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMstudio Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = {'function_declarations': [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} }]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM('vertexai/gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions in init.py sdk:  [{'function_declarations': [{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type_': 'OBJECT', 'properties': {'a': {'type_': 'NUMBER'}, 'b': {'type_': 'NUMBER'}}, 'required': ['a', 'b']}}]}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions recieves in vertex.py skd: [{'function_declarations': [{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type_': 'OBJECT', 'properties': {'a': {'type_': 'NUMBER'}, 'b': {'type_': 'NUMBER'}}, 'required': ['a', 'b']}}]}]\n",
      "Type of functions in vertex.py skd: <class 'list'>\n",
      "Breakpoint: 1 - Model detects functions\n",
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=glm.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"function_call\": {\n",
      "                  \"name\": \"multiply\",\n",
      "                  \"args\": {\n",
      "                    \"a\": 9.0,\n",
      "                    \"b\": 10.0\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": 1,\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": 10,\n",
      "              \"probability\": 1,\n",
      "              \"blocked\": false\n",
      "            },\n",
      "            {\n",
      "              \"category\": 7,\n",
      "              \"probability\": 1,\n",
      "              \"blocked\": false\n",
      "            },\n",
      "            {\n",
      "              \"category\": 9,\n",
      "              \"probability\": 1,\n",
      "              \"blocked\": false\n",
      "            },\n",
      "            {\n",
      "              \"category\": 8,\n",
      "              \"probability\": 1,\n",
      "              \"blocked\": false\n",
      "            }\n",
      "          ],\n",
      "          \"token_count\": 0,\n",
      "          \"grounding_attributions\": []\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 57,\n",
      "        \"candidates_token_count\": 19,\n",
      "        \"total_token_count\": 76\n",
      "      }\n",
      "    }),\n",
      ")\n",
      "Breakpoint: 2 -  Model Generates function response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogoazevedo/Documents/GitHub/LLMstudio/llmstudio/engine/__init__.py\", line 132, in chat_handler\n",
      "    return await provider_instance.chat(await request.json())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogoazevedo/Documents/GitHub/LLMstudio/llmstudio/engine/providers/provider.py\", line 84, in chat\n",
      "    return JSONResponse(content=await response_handler.__anext__())\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogoazevedo/Documents/GitHub/LLMstudio/llmstudio/engine/providers/provider.py\", line 116, in handle_response\n",
      "    async for chunk in self.parse_response(response, request=request):\n",
      "  File \"/Users/diogoazevedo/Documents/GitHub/LLMstudio/llmstudio/engine/providers/vertex.py\", line 81, in parse_response\n",
      "    delta=ChoiceDelta(content=chunk.text, role=\"assistant\"),\n",
      "                              ^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/generativeai/types/generation_types.py\", line 408, in text\n",
      "    raise ValueError(\n",
      "ValueError: The `response.text` quick accessor only works for simple (single-`Part`) text responses. This response is not simple text. Use the `result.parts` accessor or the full `result.candidates[index].content.parts` lookup instead.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: http://localhost:50001/api/engine/chat/vertexai",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhat is 9 times 10?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcalculator\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/__init__.py:63\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, input, is_stream, retries, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunctions in init.py sdk: \u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     34\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_HOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_PORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/engine/chat/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:50001/api/engine/chat/vertexai"
     ]
    }
   ],
   "source": [
    "llm.chat('What is 9 times 10?', functions = [calculator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = {'function_declarations': [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} }]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(calculator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Tracking on http://localhost:50002 Running LLMstudio Engine on http://localhost:50001 \n",
      "\n",
      "Functions recieves in vertex.py skd: [{'function_declarations': [{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type_': 'OBJECT', 'properties': {'a': {'type_': 'NUMBER'}, 'b': {'type_': 'NUMBER'}}, 'required': ['a', 'b']}}]}]\n",
      "Type of functions in vertex.py skd: <class 'list'>\n",
      "Breakpoint: 1 - Model detects functions\n",
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=glm.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"function_call\": {\n",
      "                  \"name\": \"multiply\",\n",
      "                  \"args\": {\n",
      "                    \"a\": 9.0,\n",
      "                    \"b\": 10.0\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": 1,\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": 9,\n",
      "              \"probability\": 1,\n",
      "              \"blocked\": false\n",
      "            },\n",
      "            {\n",
      "              \"category\": 8,\n",
      "              \"probability\": 1,\n",
      "              \"blocked\": false\n",
      "            },\n",
      "            {\n",
      "              \"category\": 7,\n",
      "              \"probability\": 1,\n",
      "              \"blocked\": false\n",
      "            },\n",
      "            {\n",
      "              \"category\": 10,\n",
      "              \"probability\": 1,\n",
      "              \"blocked\": false\n",
      "            }\n",
      "          ],\n",
      "          \"token_count\": 0,\n",
      "          \"grounding_attributions\": []\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 57,\n",
      "        \"candidates_token_count\": 19,\n",
      "        \"total_token_count\": 76\n",
      "      }\n",
      "    }),\n",
      ")\n",
      "Breakpoint: 2 -  Model Generates function response\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('vertexai/gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions in init.py sdk:  [{'function_declarations': [{'name': 'multiply', 'description': 'Returns the product of two numbers.', 'parameters': {'type_': 'OBJECT', 'properties': {'a': {'type_': 'NUMBER'}, 'b': {'type_': 'NUMBER'}}, 'required': ['a', 'b']}}]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogoazevedo/Documents/GitHub/LLMstudio/llmstudio/engine/__init__.py\", line 132, in chat_handler\n",
      "    return await provider_instance.chat(await request.json())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogoazevedo/Documents/GitHub/LLMstudio/llmstudio/engine/providers/provider.py\", line 84, in chat\n",
      "    return JSONResponse(content=await response_handler.__anext__())\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/diogoazevedo/Documents/GitHub/LLMstudio/llmstudio/engine/providers/provider.py\", line 116, in handle_response\n",
      "    async for chunk in self.parse_response(response, request=request):\n",
      "  File \"/Users/diogoazevedo/Documents/GitHub/LLMstudio/llmstudio/engine/providers/vertex.py\", line 81, in parse_response\n",
      "    delta=ChoiceDelta(content=chunk.text, role=\"assistant\"),\n",
      "                              ^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/generativeai/types/generation_types.py\", line 408, in text\n",
      "    raise ValueError(\n",
      "ValueError: The `response.text` quick accessor only works for simple (single-`Part`) text responses. This response is not simple text. Use the `result.parts` accessor or the full `result.candidates[index].content.parts` lookup instead.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: http://localhost:50001/api/engine/chat/vertexai",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhat is 9 plus 10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcalculator\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/__init__.py:63\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, input, is_stream, retries, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunctions in init.py sdk: \u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     34\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_HOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENGINE_PORT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/engine/chat/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:50001/api/engine/chat/vertexai"
     ]
    }
   ],
   "source": [
    "llm.chat('What is 9 plus 10', functions = [calculator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_declarations {\n",
       "  name: \"set_light_values\"\n",
       "  description: \"Set the brightness and color temperature of a room light. (mock API).\\n\\n    Args:\\n        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\\n        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\\n\\n    Returns:\\n        A dictionary containing the set brightness and color temperature.\\n    \"\n",
       "  parameters {\n",
       "    type_: OBJECT\n",
       "    properties {\n",
       "      key: \"color_temp\"\n",
       "      value {\n",
       "        type_: INTEGER\n",
       "      }\n",
       "    }\n",
       "    properties {\n",
       "      key: \"brightness\"\n",
       "      value {\n",
       "        type_: INTEGER\n",
       "      }\n",
       "    }\n",
       "    required: \"brightness\"\n",
       "    required: \"color_temp\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proto_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"function_declarations {\\n  name: \\\"set_light_values\\\"\\n  description: \\\"Set the brightness and color temperature of a room light. (mock API).\\\\n\\\\n    Args:\\\\n        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\\\\n        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\\\\n\\\\n    Returns:\\\\n        A dictionary containing the set brightness and color temperature.\\\\n    \\\"\\n  parameters {\\n    type_: OBJECT\\n    properties {\\n      key: \\\"color_temp\\\"\\n      value {\\n        type_: INTEGER\\n      }\\n    }\\n    properties {\\n      key: \\\"brightness\\\"\\n      value {\\n        type_: INTEGER\\n      }\\n    }\\n    required: \\\"brightness\\\"\\n    required: \\\"color_temp\\\"\\n  }\\n}\\n\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "proto_function_json = json.dumps(proto_function, default=str)\n",
    "print(proto_function_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Unknown field for Tool: DESCRIPTOR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming proto_function is your protobuf message\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m proto_function_json \u001b[38;5;241m=\u001b[39m \u001b[43mMessageToJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(proto_function_json)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/protobuf/json_format.py:107\u001b[0m, in \u001b[0;36mMessageToJson\u001b[0;34m(message, including_default_value_fields, preserving_proto_field_name, indent, sort_keys, use_integers_for_enums, descriptor_pool, float_precision, ensure_ascii)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts protobuf message to JSON format.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m  A string containing the JSON formatted protocol buffer message.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m printer \u001b[38;5;241m=\u001b[39m _Printer(\n\u001b[1;32m    102\u001b[0m     including_default_value_fields,\n\u001b[1;32m    103\u001b[0m     preserving_proto_field_name,\n\u001b[1;32m    104\u001b[0m     use_integers_for_enums,\n\u001b[1;32m    105\u001b[0m     descriptor_pool,\n\u001b[1;32m    106\u001b[0m     float_precision\u001b[38;5;241m=\u001b[39mfloat_precision)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToJsonString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/protobuf/json_format.py:174\u001b[0m, in \u001b[0;36m_Printer.ToJsonString\u001b[0;34m(self, message, indent, sort_keys, ensure_ascii)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mToJsonString\u001b[39m(\u001b[38;5;28mself\u001b[39m, message, indent, sort_keys, ensure_ascii):\n\u001b[0;32m--> 174\u001b[0m   js \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_MessageToJsonObject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mdumps(\n\u001b[1;32m    176\u001b[0m       js, indent\u001b[38;5;241m=\u001b[39mindent, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/google/protobuf/json_format.py:180\u001b[0m, in \u001b[0;36m_Printer._MessageToJsonObject\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_MessageToJsonObject\u001b[39m(\u001b[38;5;28mself\u001b[39m, message):\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts message to an object according to Proto3 JSON Specification.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m   message_descriptor \u001b[38;5;241m=\u001b[39m \u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\n\u001b[1;32m    181\u001b[0m   full_name \u001b[38;5;241m=\u001b[39m message_descriptor\u001b[38;5;241m.\u001b[39mfull_name\n\u001b[1;32m    182\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _IsWrapperMessage(message_descriptor):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/proto/message.py:758\u001b[0m, in \u001b[0;36mMessage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    756\u001b[0m (key, pb_type) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pb_type_from_key(key)\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pb_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown field for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, key)\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    761\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pb, key)\n\u001b[1;32m    762\u001b[0m marshal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mmarshal\n",
      "\u001b[0;31mAttributeError\u001b[0m: Unknown field for Tool: DESCRIPTOR"
     ]
    }
   ],
   "source": [
    "from google.protobuf.json_format import MessageToJson\n",
    "import json\n",
    "\n",
    "# Assuming proto_function is your protobuf message\n",
    "proto_function_json = MessageToJson(proto_function)\n",
    "print(proto_function_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_declarations {\n",
      "  name: \"set_light_values\"\n",
      "  description: \"Set the brightness and color temperature of a room light. (mock API).\\n\\n    Args:\\n        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\\n        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\\n\\n    Returns:\\n        A dictionary containing the set brightness and color temperature.\\n    \"\n",
      "  parameters {\n",
      "    type_: OBJECT\n",
      "    properties {\n",
      "      key: \"color_temp\"\n",
      "      value {\n",
      "        type_: INTEGER\n",
      "      }\n",
      "    }\n",
      "    properties {\n",
      "      key: \"brightness\"\n",
      "      value {\n",
      "        type_: INTEGER\n",
      "      }\n",
      "    }\n",
      "    required: \"brightness\"\n",
      "    required: \"color_temp\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Error: Unknown field for Tool: DESCRIPTOR\n"
     ]
    }
   ],
   "source": [
    "from google.protobuf.json_format import MessageToJson\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Assuming proto_function is your protobuf message\n",
    "proto_function = model_1._tools.to_proto()[0]\n",
    "print(proto_function)\n",
    "\n",
    "# Convert the protobuf message to JSON\n",
    "try:\n",
    "    proto_function_json = MessageToJson(proto_function)\n",
    "    print(proto_function_json)\n",
    "except AttributeError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
